{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import lincoln\n",
    "from lincoln.numpy.layers import Dense\n",
    "from lincoln.numpy.losses import SoftmaxCrossEntropy, MeanSquaredError, SoftmaxCrossEntropyComplex\n",
    "from lincoln.numpy.optimizers import Optimizer, SGD, SGDMomentum\n",
    "from lincoln.numpy.activations import Sigmoid, Tanh, Linear, ReLU\n",
    "from lincoln.numpy.network import NeuralNetwork\n",
    "from lincoln.numpy.train import Trainer\n",
    "from lincoln.data import mnist\n",
    "from lincoln.numpy.layers import Conv2D\n",
    "from lincoln.np_utils import softmax\n",
    "\n",
    "X_train, y_train, X_test, y_test = mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)\n",
    "X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_conv, X_test_conv = X_train.reshape(-1, 1, 28, 28), X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(y_train)\n",
    "train_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][y_train[i]] = 1\n",
    "\n",
    "num_labels = len(y_test)\n",
    "test_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_model(model, test_set):\n",
    "    return print(f'''The model validation accuracy is: \n",
    "    {np.equal(np.argmax(model.forward(test_set, inference=True), axis=1), y_test).sum() * 100.0 / test_set.shape[0]:.2f}%''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I show a convolutional neural network using the convolution operation implemented using a batch matrix multiply. See the bottom of the [Code](Code.ipynb) notebook for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.041423650264733"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(70765 / 275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss 31.19150189374252\n",
      "batch 10 loss 14.150390490520444\n",
      "batch 20 loss 8.507022910130539\n",
      "batch 30 loss 9.816084605553888\n",
      "batch 40 loss 2.70693890403282\n",
      "batch 50 loss 5.039137194847854\n",
      "batch 60 loss 3.8413085692381324\n",
      "batch 70 loss 8.476435226857522\n",
      "batch 80 loss 5.382950790290696\n",
      "batch 90 loss 2.3409849435175505\n",
      "batch 100 loss 3.7577967015822247\n",
      "Validation accuracy after 100 batches is 87.34%\n",
      "batch 110 loss 8.835042029580162\n",
      "batch 120 loss 6.248299860767069\n",
      "batch 130 loss 6.094304954272118\n",
      "batch 140 loss 2.2963053850591852\n",
      "batch 150 loss 9.372588854516968\n",
      "batch 160 loss 4.553819489115189\n",
      "batch 170 loss 2.7017474375531543\n",
      "batch 180 loss 4.144653939799545\n",
      "batch 190 loss 5.302946416009569\n",
      "batch 200 loss 6.2159890006486505\n",
      "Validation accuracy after 200 batches is 85.91%\n",
      "batch 210 loss 6.197134213166534\n",
      "batch 220 loss 4.83542921877168\n",
      "batch 230 loss 5.437383165865875\n",
      "batch 240 loss 3.5524872075073803\n",
      "batch 250 loss 3.4589611479364204\n",
      "batch 260 loss 6.728340009816592\n",
      "batch 270 loss 8.225692921408275\n",
      "batch 280 loss 4.529909375835956\n",
      "batch 290 loss 2.7631021246847842\n",
      "batch 300 loss 2.5688573224878715\n",
      "Validation accuracy after 300 batches is 90.50%\n",
      "batch 310 loss 4.5309261003178145\n",
      "batch 320 loss 4.670197299404557\n",
      "batch 330 loss 2.072362267202427\n",
      "batch 340 loss 2.7631030923330453\n",
      "batch 350 loss 1.8952127652675952\n",
      "batch 360 loss 5.17261794873893\n",
      "batch 370 loss 7.619444566123909\n",
      "batch 380 loss 3.183522682537328\n",
      "batch 390 loss 3.9841947468687704\n",
      "batch 400 loss 3.7114054314281453\n",
      "Validation accuracy after 400 batches is 90.65%\n",
      "batch 410 loss 1.3818032462942236\n",
      "batch 420 loss 3.458777915153967\n",
      "batch 430 loss 2.6434544346818076\n",
      "batch 440 loss 5.029467004758276\n",
      "batch 450 loss 6.7755710286131325\n",
      "batch 460 loss 3.979756152296614\n",
      "batch 470 loss 2.7631640653779628\n",
      "batch 480 loss 4.181727159304303\n",
      "batch 490 loss 2.5034388945024477\n",
      "batch 500 loss 4.281609988043863\n",
      "Validation accuracy after 500 batches is 87.96%\n",
      "batch 510 loss 4.804136638909201\n",
      "batch 520 loss 2.7631021233449835\n",
      "batch 530 loss 0.9333283234121263\n",
      "batch 540 loss 2.3887838079956003\n",
      "batch 550 loss 1.8564541300193327\n",
      "batch 560 loss 2.988986057309694\n",
      "batch 570 loss 4.150230540572257\n",
      "batch 580 loss 4.144818307168476\n",
      "batch 590 loss 3.4538776516812293\n",
      "batch 600 loss 2.4575067041729945\n",
      "Validation accuracy after 600 batches is 89.83%\n",
      "batch 610 loss 4.605773508923405\n",
      "batch 620 loss 3.5088469590637614\n",
      "batch 630 loss 4.843797820668553\n",
      "batch 640 loss 4.827955135548562\n",
      "batch 650 loss 2.005493475173771\n",
      "batch 660 loss 4.745663602291793\n",
      "batch 670 loss 4.826229165650016\n",
      "batch 680 loss 4.650082614480861\n",
      "batch 690 loss 1.3815510666724917\n",
      "batch 700 loss 3.1842423020209183\n",
      "Validation accuracy after 700 batches is 87.20%\n",
      "batch 710 loss 3.5227898196546854\n",
      "batch 720 loss 2.7631021233449835\n",
      "batch 730 loss 6.806741903561913\n",
      "batch 740 loss 2.879979652022031\n",
      "batch 750 loss 1.8038501422247026\n",
      "batch 760 loss 1.3815510694282434\n",
      "batch 770 loss 2.1427025897011784\n",
      "batch 780 loss 3.669951356665799\n",
      "batch 790 loss 3.0207656705018584\n",
      "batch 800 loss 2.172526307773005\n",
      "Validation accuracy after 800 batches is 90.16%\n",
      "batch 810 loss 4.828680362983919\n",
      "batch 820 loss 2.6959772094324537\n",
      "batch 830 loss 5.769190583267376\n",
      "batch 840 loss 7.800582662404006\n",
      "batch 850 loss 4.363377747970372\n",
      "batch 860 loss 0.08099487534376877\n",
      "batch 870 loss 4.574553563957311\n",
      "batch 880 loss 3.4538777113811867\n",
      "batch 890 loss 3.432486929005662\n",
      "batch 900 loss 3.453877651681229\n",
      "Validation accuracy after 900 batches is 89.84%\n",
      "batch 910 loss 5.893388933806041\n",
      "batch 920 loss 5.458197248998498\n",
      "batch 930 loss 2.424583742064365\n",
      "batch 940 loss 6.002163616462233\n",
      "batch 950 loss 2.85564947525628\n",
      "batch 960 loss 4.313433349706659\n",
      "batch 970 loss 1.9493318663298638\n",
      "batch 980 loss 6.689602411442138\n",
      "batch 990 loss 7.692227582930405\n",
      "Validation loss after 1 epochs is 3.593\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Conv2D(out_channels=16,\n",
    "                   param_size=5,\n",
    "                   dropout=0.8,\n",
    "                   weight_init=\"glorot\",\n",
    "                   flatten=True,\n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190402)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr = 0.1, momentum=0.9))\n",
    "trainer.fit(X_train_conv, train_labels, X_test_conv, test_labels,\n",
    "            epochs = 1,\n",
    "            eval_every = 1,\n",
    "            seed=20190402,\n",
    "            batch_size=60,\n",
    "            conv_testing=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: \n",
      "    89.37%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss 33.216544562324664\n",
      "batch 10 loss 6.721844280652351\n",
      "batch 20 loss 8.171938545760652\n",
      "batch 30 loss 7.790772211152649\n",
      "batch 40 loss 2.763450996757522\n",
      "batch 50 loss 4.87810481499189\n",
      "batch 60 loss 4.573422932170181\n",
      "batch 70 loss 6.217209401215738\n",
      "batch 80 loss 4.482600310761919\n",
      "batch 90 loss 1.559477211993213\n",
      "batch 100 loss 4.156000259467634\n",
      "Validation accuracy after 100 batches is 86.02%\n",
      "batch 110 loss 7.602353756623126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-acc307e2872f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20190402\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             conv_testing=True);\n\u001b[0m",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_test, y_test, epochs, eval_every, batch_size, seed, single_output, restart, early_stopping, conv_testing)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/network.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, X_batch, y_batch, inference)\u001b[0m\n\u001b[1;32m     74\u001b[0m                     inference: bool = False) -> float:\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_batch, inference)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mX_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mX_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, inference)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0moperation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, inference)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m_output\u001b[0;34m(self, inference)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         patches_reshaped = (patches\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m_get_image_patches\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m     52\u001b[0m     def _get_image_patches(self,\n\u001b[1;32m     53\u001b[0m                            input_: ndarray):\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mimgs_batch_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_2d_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mimg_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_batch_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m     def _get_image_patches(self,\n\u001b[1;32m     53\u001b[0m                            input_: ndarray):\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mimgs_batch_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_2d_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mimg_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_batch_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m_pad_2d_channel\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0minp\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         '''\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_2d_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     def _get_image_patches(self,\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0minp\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         '''\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_2d_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     def _get_image_patches(self,\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m_pad_2d_obs\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mInput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0mdimensional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         '''\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0minp_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_1d_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_pad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m_pad_1d_batch\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     19\u001b[0m     def _pad_1d_batch(self,\n\u001b[1;32m     20\u001b[0m                       inp: ndarray) -> ndarray:\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     def _pad_1d_batch(self,\n\u001b[1;32m     20\u001b[0m                       inp: ndarray) -> ndarray:\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/conv.py\u001b[0m in \u001b[0;36m_pad_1d\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pad_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Conv2D(out_channels=32,\n",
    "                   param_size=5,\n",
    "                   dropout=0.8,\n",
    "                   weight_init=\"glorot\",\n",
    "                   flatten=True,\n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190402)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr = 0.1, momentum=0.9))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 1,\n",
    "            eval_every = 1,\n",
    "            seed=20190402,\n",
    "            batch_size=60,\n",
    "            conv_testing=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to our best fully connected one layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss 3.1228996380817486\n",
      "batch 10 loss 0.8932136825185367\n",
      "batch 20 loss 0.660195193867239\n",
      "batch 30 loss 0.5698901122343973\n",
      "batch 40 loss 0.4018894515299986\n",
      "batch 50 loss 0.44525706826989125\n",
      "batch 60 loss 1.2854861072936496\n",
      "batch 70 loss 0.46844966716006137\n",
      "batch 80 loss 0.48728355225641284\n",
      "batch 90 loss 0.8392205618217854\n",
      "batch 100 loss 0.7123218138821693\n",
      "Validation accuracy after 100 batches is 86.99%\n",
      "batch 110 loss 0.8647573972048214\n",
      "batch 120 loss 0.5344877210323652\n",
      "batch 130 loss 1.0108547562185006\n",
      "batch 140 loss 0.67596169501226\n",
      "batch 150 loss 0.550133359711308\n",
      "batch 160 loss 0.5832240401553612\n",
      "batch 170 loss 0.5093058556547023\n",
      "batch 180 loss 0.9334907163600867\n",
      "batch 190 loss 0.7831829932837465\n",
      "batch 200 loss 0.6795085067088273\n",
      "Validation accuracy after 200 batches is 86.62%\n",
      "batch 210 loss 0.11708777127774148\n",
      "batch 220 loss 0.3458666311780603\n",
      "batch 230 loss 0.696441179800066\n",
      "batch 240 loss 0.6271036770305358\n",
      "batch 250 loss 1.2383168801567237\n",
      "batch 260 loss 0.9830951369735401\n",
      "batch 270 loss 0.5378946193489628\n",
      "batch 280 loss 0.6439097582143309\n",
      "batch 290 loss 0.3830001342898586\n",
      "batch 300 loss 0.7925827924998263\n",
      "Validation accuracy after 300 batches is 87.79%\n",
      "batch 310 loss 0.37360142806642826\n",
      "batch 320 loss 0.7539058894034618\n",
      "batch 330 loss 0.6381601139096537\n",
      "batch 340 loss 0.7230373359462376\n",
      "batch 350 loss 0.56847256964536\n",
      "batch 360 loss 0.48558366803617137\n",
      "batch 370 loss 0.7295682432284667\n",
      "batch 380 loss 0.7953168891777805\n",
      "batch 390 loss 0.30808471171151275\n",
      "batch 400 loss 0.741989474338459\n",
      "Validation accuracy after 400 batches is 87.50%\n",
      "batch 410 loss 0.4119464677243635\n",
      "batch 420 loss 0.45059561358406863\n",
      "batch 430 loss 0.2310035435034484\n",
      "batch 440 loss 0.7152860139323967\n",
      "batch 450 loss 0.7172123737213957\n",
      "batch 460 loss 0.3814174228468519\n",
      "batch 470 loss 0.4040845975025121\n",
      "batch 480 loss 0.5384220463533466\n",
      "batch 490 loss 0.5578594221773467\n",
      "batch 500 loss 0.6184002532155818\n",
      "Validation accuracy after 500 batches is 89.15%\n",
      "batch 510 loss 0.2736328191198972\n",
      "batch 520 loss 0.7760329822365436\n",
      "batch 530 loss 0.40211444050233414\n",
      "batch 540 loss 0.5470252040404197\n",
      "batch 550 loss 0.546561872323866\n",
      "batch 560 loss 0.2843878622711272\n",
      "batch 570 loss 0.496767778942253\n",
      "batch 580 loss 0.598218774303091\n",
      "batch 590 loss 0.5285505326557376\n",
      "batch 600 loss 0.4648469890102854\n",
      "Validation accuracy after 600 batches is 88.83%\n",
      "batch 610 loss 1.0596769062899833\n",
      "batch 620 loss 0.4687865346788215\n",
      "batch 630 loss 0.8131924097657534\n",
      "batch 640 loss 0.6031537359856762\n",
      "batch 650 loss 0.3209208378833322\n",
      "batch 660 loss 0.2228716697393528\n",
      "batch 670 loss 0.5499286381531563\n",
      "batch 680 loss 0.4047187205962384\n",
      "batch 690 loss 0.5739389707990027\n",
      "batch 700 loss 0.5211981896983872\n",
      "Validation accuracy after 700 batches is 90.36%\n",
      "batch 710 loss 0.2692816789474053\n",
      "batch 720 loss 0.6255058391902616\n",
      "batch 730 loss 0.18684118701881566\n",
      "batch 740 loss 0.7431886576163593\n",
      "batch 750 loss 0.9127677075477959\n",
      "batch 760 loss 0.5860687894382982\n",
      "batch 770 loss 0.27507915856712145\n",
      "batch 780 loss 0.5063988579164959\n",
      "batch 790 loss 0.6566156479141589\n",
      "batch 800 loss 0.7002345520689149\n",
      "Validation accuracy after 800 batches is 89.43%\n",
      "batch 810 loss 0.4141510304189733\n",
      "batch 820 loss 0.3439572583072195\n",
      "batch 830 loss 0.4433644806065652\n",
      "batch 840 loss 0.5519941434055712\n",
      "batch 850 loss 0.6123524524002908\n",
      "batch 860 loss 0.43688809202839096\n",
      "batch 870 loss 0.3639672061759655\n",
      "batch 880 loss 0.4971641873295436\n",
      "batch 890 loss 0.2048734301991138\n",
      "batch 900 loss 0.7002029287584212\n",
      "Validation accuracy after 900 batches is 90.42%\n",
      "batch 910 loss 0.284963103232843\n",
      "batch 920 loss 0.5551058831236855\n",
      "batch 930 loss 0.35272915621999146\n",
      "batch 940 loss 0.5589780676417866\n",
      "batch 950 loss 0.5785046984013956\n",
      "batch 960 loss 0.28540749489731865\n",
      "batch 970 loss 0.7124764760745094\n",
      "batch 980 loss 0.2947277910959006\n",
      "batch 990 loss 0.3417088752308849\n",
      "Validation loss after 1 epochs is 0.645\n",
      "The model validation accuracy is: \n",
      "    89.15%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"glorot\",\n",
    "                  dropout=0.8),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear(),\n",
    "                  weight_init=\"glorot\")],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190119)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 1,\n",
    "       eval_every = 1,\n",
    "       seed=20190119,\n",
    "           batch_size=60,\n",
    "           early_stopping=True,\n",
    "           conv_testing=True);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
