{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(x+1 for x in (1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = ['a','b','c']\n",
    "l2 = ['d','e','f']\n",
    "l3 = ['g','h','i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "a d g\n",
      "1\n",
      "b e h\n",
      "2\n",
      "c f i\n"
     ]
    }
   ],
   "source": [
    "for i, (a, b, c) in enumerate(zip(l1, l2, l3)):\n",
    "    print(i)\n",
    "    print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a1 b1\n",
      "1 a2 b2\n",
      "2 a3 b3\n"
     ]
    }
   ],
   "source": [
    "alist = ['a1', 'a2', 'a3']\n",
    "blist = ['b1', 'b2', 'b3']\n",
    "\n",
    "for i, (a, b) in enumerate(zip(alist, blist)):\n",
    "    print(i, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.Tensor([2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mul_(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lunch and Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro/Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your current understanding of RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LSTM_next_character.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_unrolling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"RNNs have a hidden state that feeds back into the cell at the next time step\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is actually going on?\n",
    "\n",
    "### Example with sequence length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the backwards pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_in_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about deep learning?\n",
    "\n",
    "### What would an RNN with multiple layers look like?\n",
    "\n",
    "![](img/rnn_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the backward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about LSTMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_olah.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Forward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_forward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flowing Backward Through an LSTM-based Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or equivalently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm_backward_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I used [this minimal example](https://gist.github.com/karpathy/d4dee566867f8291f086) from Andrej Karpathy as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def row_softmax(input_array):    \n",
    "    a_exp = np.exp(input_array)\n",
    "    row_sums = a_exp.sum(axis=1)\n",
    "    new_matrix = a_exp / row_sums[:, np.newaxis]\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Param` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Param:\n",
    "    '''\n",
    "    A class that holds weight matrices along with their derivatives and momentum.\n",
    "    '''\n",
    "    def __init__(self, value):\n",
    "        '''\n",
    "        param value: numpy array, two dimensional, shape of the weight matrix\n",
    "        '''\n",
    "        self.value = value\n",
    "        self.deriv = np.zeros_like(value) #derivative\n",
    "        self.momentum = np.zeros_like(value) #momentum for AdaGrad\n",
    "        \n",
    "    def clear_gradient(self):\n",
    "        '''\n",
    "        Resets the value of the derivative\n",
    "        '''\n",
    "        self.deriv = np.zeros_like(self.value) #derivative\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        '''\n",
    "        Clips the derivative, setting its min value to -1 and its max value to 1.\n",
    "        '''\n",
    "        self.deriv = np.clip(self.deriv, -2, 2, out=self.deriv)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to AdaGrad rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.momentum += self.deriv * self.deriv # Calculate sum of gradients\n",
    "        self.value += -(learning_rate * self.deriv / np.sqrt(self.momentum + 1e-8))\n",
    "        \n",
    "    def update_sgd(self, learning_rate):\n",
    "        '''\n",
    "        Update parameter values according to stochastic gradient descent rules.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.value -= learning_rate * self.deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Params` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Params:\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        The parameters to be used when updating the values in an LSTM_Layer, which is a layer\n",
    "        of LSTM cells stretched out over time.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the layer. This can be \n",
    "        different for each layer.\n",
    "        param vocab_size: int - the number of characters in the vocabulary that we are predicting\n",
    "        the next character of.\n",
    "        Note: the shape of these weight matrices assumes that the data will be fed in as \"rows\", \n",
    "        meaning that each data point will be represented by a numpy array of shape (1, vocab_size)\n",
    "        '''\n",
    "        self.stack_size = hidden_size + vocab_size\n",
    "        \n",
    "        self.W_f = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_i = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_c = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_o = LSTM_Param(np.random.normal(size=(self.stack_size, hidden_size), loc=0, scale=0.1))\n",
    "        self.W_v = LSTM_Param(np.random.normal(size=(hidden_size, vocab_size), loc=0, scale=0.1))\n",
    "        \n",
    "        '''\n",
    "        Biases always have the dimensions of the output of the transformation that they are being added to.\n",
    "        '''\n",
    "        self.B_f = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_i = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_c = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_o = LSTM_Param(np.zeros((1, hidden_size)))\n",
    "        self.B_v = LSTM_Param(np.zeros((1, vocab_size)))\n",
    "\n",
    "        \n",
    "    def all_params(self):\n",
    "        '''\n",
    "        Returns a list of all the parameters for easy iteration\n",
    "        '''\n",
    "        return [self.W_f, self.W_i, self.W_c, self.W_o, self.W_v, \n",
    "                self.B_f, self.B_i, self.B_c, self.B_o, self.B_v]\n",
    "        \n",
    "    def clear_gradients(self):\n",
    "        '''\n",
    "        Clears all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clear_gradient()\n",
    "        \n",
    "    def clip_gradients(self):\n",
    "        '''\n",
    "        Clips all the gradients\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            param.clip_gradient()       \n",
    "       \n",
    "    def update_params(self, learning_rate, method=\"ada\"):\n",
    "        '''\n",
    "        Updates all the parameters according to the \"AdaGrad\" rule.\n",
    "        '''\n",
    "        for param in self.all_params():\n",
    "            if method == \"ada\":\n",
    "                param.update(learning_rate)  \n",
    "            elif method == \"sgd\":\n",
    "                param.update_sgd(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Node:\n",
    "    '''\n",
    "    An LSTM Node performing the role of the \"circles\" in the diagrams above.\n",
    "    It takes has two methods:\n",
    "    1. \"forward\" which takes in: \n",
    "        * An observation, \"X\"\n",
    "        * A hidden state from the prior time step, \"H_prev\"\n",
    "        * A cell state from the prior time step, \"C_prev\"\n",
    "        And returns:\n",
    "        * The output \"X_out\" to be passed on to the next layer.\n",
    "        * The output \"H\" to be passed into the next node as the hidden state\n",
    "        * The output \"C\" to be passed into the next node as the cell state\n",
    "    2. \"backward\" which takes in: \n",
    "        * The gradient of the output with respect to the loss, \"out_grad\"\n",
    "        * The gradient of the hidden state with respect to the loss, \"h_grad\"\n",
    "        * The gradient of the cell state with respect to the loss, \"c_grad\"\n",
    "        And returns:\n",
    "        * The gradient of the output of the same time step in the prior layer - the circle \"below\" in the \n",
    "        drawings above - \"dx_grad\"\n",
    "        * The gradient of the hidden state from the prior time step, \"dh_grad\" \n",
    "        * The gradient of the cell state from the prior time step, \"dc_grad\" \n",
    "    \n",
    "    It uses as parameters, not its own parameters, but the parameters from the LSTM_Layer that it is a part of.\n",
    "        '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, H_prev, C_prev, LSTM_Params):\n",
    "        '''\n",
    "        param x: numpy array of shape (1, vocab_size)\n",
    "        param H_prev: numpy array of shape (1, hidden_size)\n",
    "        param C_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.x_out: numpy array of shape (1, vocab_size)\n",
    "        return self.H: numpy array of shape (1, hidden_size)\n",
    "        return self.C: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        self.C_prev = C_prev\n",
    "\n",
    "        self.z = np.column_stack((x, H_prev))\n",
    "        \n",
    "        self.f = sigmoid(np.dot(self.z, LSTM_Params.W_f.value) + LSTM_Params.B_f.value)\n",
    "        self.i = sigmoid(np.dot(self.z, LSTM_Params.W_i.value) + LSTM_Params.B_i.value)\n",
    "        self.C_bar = tanh(np.dot(self.z, LSTM_Params.W_c.value) + LSTM_Params.B_c.value)\n",
    "\n",
    "        self.C = self.f * C_prev + self.i * self.C_bar\n",
    "        self.o = sigmoid(np.dot(self.z, LSTM_Params.W_o.value) + LSTM_Params.B_o.value)\n",
    "        self.H = self.o * tanh(self.C)\n",
    "\n",
    "        self.x_out = np.dot(self.H, LSTM_Params.W_v.value) + LSTM_Params.B_v.value\n",
    "        \n",
    "        return self.x_out, self.H, self.C \n",
    "\n",
    "\n",
    "    def backward(self, loss_grad, dh_next, dC_next, LSTM_Params):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (1, vocab_size)\n",
    "        param dh_next: numpy array of shape (1, hidden_size)\n",
    "        param dC_next: numpy array of shape (1, hidden_size)\n",
    "        param LSTM_Params: LSTM_Params object\n",
    "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
    "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        LSTM_Params.W_v.deriv += np.dot(self.H.T, loss_grad)\n",
    "        LSTM_Params.B_v.deriv += loss_grad\n",
    "\n",
    "        dh = np.dot(loss_grad, LSTM_Params.W_v.value.T)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(self.C)\n",
    "        do_int = dsigmoid(self.o) * do\n",
    "        LSTM_Params.W_o.deriv += np.dot(self.z.T, do_int)\n",
    "        LSTM_Params.B_o.deriv += do_int\n",
    "\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * self.o * dtanh(tanh(self.C))\n",
    "        dC_bar = dC * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar) * dC_bar\n",
    "        LSTM_Params.W_c.deriv += np.dot(self.z.T, dC_bar_int)\n",
    "        LSTM_Params.B_c.deriv += dC_bar_int\n",
    "\n",
    "        di = dC * self.C_bar\n",
    "        di_int = dsigmoid(self.i) * di\n",
    "        LSTM_Params.W_i.deriv += np.dot(self.z.T, di_int)\n",
    "        LSTM_Params.B_i.deriv += di_int\n",
    "\n",
    "        df = dC * self.C_prev\n",
    "        df_int = dsigmoid(self.f) * df\n",
    "        LSTM_Params.W_f.deriv += np.dot(self.z.T, df_int)\n",
    "        LSTM_Params.B_f.deriv += df_int\n",
    "\n",
    "        dz = (np.dot(df_int, LSTM_Params.W_f.value.T)\n",
    "             + np.dot(di_int, LSTM_Params.W_i.value.T)\n",
    "             + np.dot(dC_bar_int, LSTM_Params.W_c.value.T)\n",
    "             + np.dot(do_int, LSTM_Params.W_o.value.T))\n",
    "        dx_prev = dz[:, :self.vocab_size]\n",
    "        dH_prev = dz[:, self.vocab_size:]\n",
    "        dC_prev = self.f * dC\n",
    "        \n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Layer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Layer:\n",
    "    '''\n",
    "    Corresponds to a \"row\" of circles in the diagrams above. \n",
    "    We initialize a layer with a number of \"Nodes\" equal to the length of the sequences we passin.\n",
    "    Again, the key methods are \"forward\" and \"backward\".\n",
    "    1. \"forward\":\n",
    "        * Passes data forward through all the nodes in the layer\n",
    "        * Sets the hidden and cell states outputted by the last node to be those that will be passed in when the\n",
    "        next sequence is fed through the layer\n",
    "        * Passes the appropriate data on to the next layer in the network\n",
    "    2. \"backward\":\n",
    "        * Passes gradient backward through all the nodes in the layer\n",
    "        * Sets the hidden state outputted by the last node equal to the node \n",
    "        * Passes the appropriate gradient on to the previous layer in the network\n",
    "    '''\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.nodes = [LSTM_Node(hidden_size, vocab_size) for x in range(sequence_length)]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = LSTM_Params(hidden_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x_seq_in):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        num_chars = x_seq_in.shape[0]\n",
    "        \n",
    "        x_seq_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in range(num_chars):\n",
    "\n",
    "            # Need to cast this one dimensional numpy array to two dimensions\n",
    "            # for the matrix multiplication to work\n",
    "            x_in = np.array(x_seq_in[t, :], ndmin=2)\n",
    "            \n",
    "            y_out, H_in, C_in = self.nodes[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_seq_out[t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''\n",
    "        dh_next = np.zeros_like(self.start_H) #dh from the next character\n",
    "        dC_next = np.zeros_like(self.start_C) #dc from the next character\n",
    "        \n",
    "        num_chars = loss_grad.shape[0]\n",
    "        \n",
    "        loss_grad_out = np.zeros((num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            # Need to cast this one dimensional numpy array to two dimensions\n",
    "            # for the matrix multiplication to work\n",
    "            loss_grad_in = np.array(loss_grad[t, :], ndmin=2)\n",
    "\n",
    "            grad_out, dh_next, dC_next = \\\n",
    "                self.nodes[t].backward(loss_grad_in, dh_next, dC_next, self.params)\n",
    "        \n",
    "            loss_grad_out[t, :] = grad_out\n",
    "        \n",
    "        return loss_grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM_Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model:\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, num_layers, sequence_length, vocab_size, hidden_size, learning_rate):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = [LSTM_Layer(sequence_length, vocab_size, hidden_size, learning_rate) for i in range(num_layers)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "\n",
    "    def _generate_one_hot_array(self, sequence):\n",
    "        '''\n",
    "        param sequence - sequence of indices of characters of length sequence_length.\n",
    "        return batch - numpy array of shape (sequence_length, vocab_size)\n",
    "        '''       \n",
    "        sequence_length = len(sequence)\n",
    "        batch = np.zeros((sequence_length, self.vocab_size))\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "        \n",
    "        return batch\n",
    "        \n",
    "    def loss(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "        loss = np.sum((x_batch_out - y_batch) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def loss_gradient(self, x_batch_out, y_batch):\n",
    "        '''\n",
    "        param x_batch_out: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        return -1.0 * (y_batch - x_batch_out)\n",
    "\n",
    "    def backward(self, loss_grad):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, inputs, targets):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        \n",
    "        x_batch_in = self._generate_one_hot_array(inputs)\n",
    "        \n",
    "        x_batch_out = self.forward(x_batch_in)\n",
    "        \n",
    "        x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "        y_batch = self._generate_one_hot_array(targets)\n",
    "        \n",
    "        loss = self.loss(x_softmax, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss_gradient(x_softmax, y_batch)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.params.clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.params.clip_gradients()  \n",
    "            layer.params.update_params(layer.learning_rate)\n",
    "            \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Character_generator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_generator:\n",
    "    '''\n",
    "    Takes in a text file and a model, and starts generating characters.\n",
    "    '''\n",
    "    def __init__(self, text_file, LSTM_Model):\n",
    "        '''\n",
    "        param text_file: path to text file containing characters that we are going to generate text to mimic\n",
    "        param LSTM_Model: the LSTM_Model that we are using to generate the text \n",
    "        '''\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = LSTM_Model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "    \n",
    "    def _generate_inputs_targets(self, start_pos):\n",
    "        '''\n",
    "        Given a start position, generates a list of inputs and targets.\n",
    "        param start_pos: int - index of start position in characters \n",
    "        return inputs: list - list of length sequence_length with the indices of the characters forming the \n",
    "        inputs to the model \n",
    "        return targets: list - list of length sequence_length with the indices of the characters forming the \n",
    "        targets of the model\n",
    "        '''\n",
    "        inputs = ([self.char_to_idx[ch] \n",
    "                   for ch in self.data[start_pos: start_pos + self.sequence_length]])\n",
    "        targets = ([self.char_to_idx[ch] \n",
    "                    for ch in self.data[start_pos + 1: start_pos + self.sequence_length + 1]])\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "    def sample_output(self, input_char, sample_length):\n",
    "        '''\n",
    "        Generates a sample output using the current trained model.\n",
    "        param input_char: int - index of the character to use to start generating a sequence\n",
    "        param sample_length: int - the length of the sample output to generate\n",
    "        return txt: string - a string of length sample_length representing the sample output\n",
    "        '''\n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            x_batch_in = self.model._generate_one_hot_array([input_char])\n",
    "            \n",
    "            x_batch_out = sample_model.forward(x_batch_in)\n",
    "        \n",
    "            x_softmax = row_softmax(x_batch_out)\n",
    "        \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "\n",
    "    def train(self, num_iterations, sample_every=100):\n",
    "        '''\n",
    "        Trains the \"character generator\" for a number of iterations. \n",
    "        Each \"iteration\" feeds a batch size of 1 through the neural network.\n",
    "        Continues until num_iterations is reached. Displays sample text generated using the latest version.\n",
    "        '''\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if start_pos + self.sequence_length > len(self.data):\n",
    "                start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            inputs, targets = self._generate_inputs_targets(start_pos)\n",
    "            loss = self.model.single_step(inputs, targets)\n",
    "\n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            start_pos += self.sequence_length\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            num_iter += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will domes silam, and say the bey thesod ford\n",
      "Ans?\n",
      "\n",
      "CeGNTHTEK:\n",
      "Seat Ins mo nllewssqued:\n",
      "Fores and thes,\n",
      "'he well is sheald atd\n",
      "lico gasis Shiy thet loatld to prtee sway same;\n",
      "Tayses ar foad mo mhis s\n"
     ]
    }
   ],
   "source": [
    "mod = LSTM_Model(vocab_size=62, hidden_size=256, learning_rate=0.01, sequence_length=25, num_layers=1)\n",
    "character_generator = Character_generator('input.txt', mod)\n",
    "character_generator.train(10000, sample_every=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
