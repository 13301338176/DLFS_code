{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks:\n",
    "\n",
    "* PyTorch first pass\n",
    "    * `Model` with `forward` method.\n",
    "    * Manual training loop\n",
    "* PyTorch second pass\n",
    "    * `Model` with `forward` method.\n",
    "    * `Trainer` class that takes in:\n",
    "        * `Model`\n",
    "        * `Optimizer`\n",
    "        * `_Loss`\n",
    "\n",
    "Models:\n",
    "\n",
    "* Boston dataset (used for testing)\n",
    "* MNIST Conv net\n",
    "* LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from lincoln.utils import permute_data, assert_dim\n",
    "\n",
    "from lincoln.pytorch.model import PyTorchModel\n",
    "from lincoln.pytorch.train import PyTorchTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Boston model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BostonModel(\n",
      "  (fc1): Linear(in_features=13, out_features=13, bias=True)\n",
      "  (fc2): Linear(in_features=13, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BostonModel(PyTorchModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(13, 13)\n",
    "        self.fc2 = nn.Linear(13, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert_dim(x, 2)\n",
    "        \n",
    "        assert x.shape[1] == 13\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = BostonModel()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer, loss\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(X: Tensor,\n",
    "                      y: Tensor,\n",
    "                      size: int = 32) -> Tuple[Tensor]:\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for ii in range(0, N, size):\n",
    "        X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1287.3287353515625\n",
      "1 244.7203826904297\n",
      "2 98.0165023803711\n",
      "3 96.56353759765625\n",
      "4 99.84830474853516\n",
      "5 159.05905151367188\n",
      "6 145.64295959472656\n",
      "7 123.37718200683594\n",
      "8 117.31515502929688\n",
      "9 103.74791717529297\n",
      "10 98.30821228027344\n",
      "11 100.46519470214844\n",
      "12 93.91365814208984\n",
      "13 125.31230926513672\n",
      "14 165.37962341308594\n",
      "15 100.69593048095703\n",
      "16 91.53426361083984\n",
      "17 183.2888946533203\n",
      "18 93.11546325683594\n",
      "19 409.34185791015625\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "for e in range(epochs):\n",
    "    X_train, y_train = permute_data(X_train, y_train)  \n",
    "\n",
    "    batch_generator = generate_batches(X_train, y_train,\n",
    "                                       batch_size)\n",
    "\n",
    "    for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "        output = net(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "    \n",
    "    optimizer.zero_grad() \n",
    "    output = net(X_test)\n",
    "    loss = criterion(output, y_test)\n",
    "    print(e, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(409.3419, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(output, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Trainer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 0 epochs was 538.8438720703125\n",
      "The loss after 1 epochs was 480.07720947265625\n",
      "The loss after 2 epochs was 459.6590270996094\n",
      "The loss after 3 epochs was 408.1224670410156\n",
      "The loss after 4 epochs was 316.6106872558594\n",
      "The loss after 5 epochs was 260.6806945800781\n",
      "The loss after 6 epochs was 203.76148986816406\n",
      "The loss after 7 epochs was 171.43287658691406\n",
      "The loss after 8 epochs was 146.33197021484375\n",
      "The loss after 9 epochs was 130.50559997558594\n",
      "The loss after 10 epochs was 113.76214599609375\n",
      "The loss after 11 epochs was 107.16474914550781\n",
      "The loss after 12 epochs was 100.93927764892578\n",
      "The loss after 13 epochs was 93.789306640625\n",
      "The loss after 14 epochs was 95.15956115722656\n",
      "The loss after 15 epochs was 91.48625183105469\n",
      "The loss after 16 epochs was 91.11739349365234\n",
      "The loss after 17 epochs was 90.99897766113281\n",
      "The loss after 18 epochs was 90.94464111328125\n",
      "The loss after 19 epochs was 91.48057556152344\n",
      "The loss after 20 epochs was 91.81754302978516\n",
      "The loss after 21 epochs was 91.2906494140625\n",
      "The loss after 22 epochs was 91.8261947631836\n",
      "The loss after 23 epochs was 91.28895568847656\n",
      "The loss after 24 epochs was 91.06958770751953\n",
      "The loss after 25 epochs was 91.09445190429688\n",
      "The loss after 26 epochs was 92.98880767822266\n",
      "The loss after 27 epochs was 92.10770416259766\n",
      "The loss after 28 epochs was 92.66143035888672\n",
      "The loss after 29 epochs was 92.39036560058594\n",
      "The loss after 30 epochs was 92.10721588134766\n",
      "The loss after 31 epochs was 92.23113250732422\n",
      "The loss after 32 epochs was 93.3538589477539\n",
      "The loss after 33 epochs was 93.09654998779297\n",
      "The loss after 34 epochs was 93.46279907226562\n",
      "The loss after 35 epochs was 94.51795196533203\n",
      "The loss after 36 epochs was 97.57450103759766\n",
      "The loss after 37 epochs was 96.91841125488281\n",
      "The loss after 38 epochs was 96.38226318359375\n",
      "The loss after 39 epochs was 95.72388458251953\n",
      "The loss after 40 epochs was 94.34937286376953\n",
      "The loss after 41 epochs was 94.03904724121094\n",
      "The loss after 42 epochs was 95.48094177246094\n",
      "The loss after 43 epochs was 94.91233825683594\n",
      "The loss after 44 epochs was 94.73818969726562\n",
      "The loss after 45 epochs was 94.56460571289062\n",
      "The loss after 46 epochs was 94.95423889160156\n",
      "The loss after 47 epochs was 95.21220397949219\n",
      "The loss after 48 epochs was 96.60901641845703\n",
      "The loss after 49 epochs was 97.13331604003906\n",
      "The loss after 50 epochs was 95.91328430175781\n",
      "The loss after 51 epochs was 95.34435272216797\n",
      "The loss after 52 epochs was 95.62083435058594\n",
      "The loss after 53 epochs was 96.15023803710938\n",
      "The loss after 54 epochs was 94.69729614257812\n",
      "The loss after 55 epochs was 93.49938201904297\n",
      "The loss after 56 epochs was 92.79085540771484\n",
      "The loss after 57 epochs was 92.6745834350586\n",
      "The loss after 58 epochs was 92.59663391113281\n",
      "The loss after 59 epochs was 94.22821044921875\n",
      "The loss after 60 epochs was 93.68977355957031\n",
      "The loss after 61 epochs was 93.94975280761719\n",
      "The loss after 62 epochs was 94.1830062866211\n",
      "The loss after 63 epochs was 93.8392333984375\n",
      "The loss after 64 epochs was 93.60521697998047\n",
      "The loss after 65 epochs was 93.39505767822266\n",
      "The loss after 66 epochs was 94.87975311279297\n",
      "The loss after 67 epochs was 94.59381103515625\n",
      "The loss after 68 epochs was 93.69758605957031\n",
      "The loss after 69 epochs was 94.00309753417969\n",
      "The loss after 70 epochs was 93.41868591308594\n",
      "The loss after 71 epochs was 94.35761260986328\n",
      "The loss after 72 epochs was 94.67874908447266\n",
      "The loss after 73 epochs was 95.03243255615234\n",
      "The loss after 74 epochs was 94.89629364013672\n",
      "The loss after 75 epochs was 93.89852142333984\n",
      "The loss after 76 epochs was 93.9001693725586\n",
      "The loss after 77 epochs was 92.60292053222656\n",
      "The loss after 78 epochs was 93.14835357666016\n",
      "The loss after 79 epochs was 92.74488067626953\n",
      "The loss after 80 epochs was 93.20005798339844\n",
      "The loss after 81 epochs was 93.11555480957031\n",
      "The loss after 82 epochs was 92.3254165649414\n",
      "The loss after 83 epochs was 92.66609954833984\n",
      "The loss after 84 epochs was 93.48193359375\n",
      "The loss after 85 epochs was 95.04498291015625\n",
      "The loss after 86 epochs was 95.03033447265625\n",
      "The loss after 87 epochs was 94.19434356689453\n",
      "The loss after 88 epochs was 92.98031616210938\n",
      "The loss after 89 epochs was 92.76018524169922\n",
      "The loss after 90 epochs was 93.9029769897461\n",
      "The loss after 91 epochs was 96.1928482055664\n",
      "The loss after 92 epochs was 96.20381164550781\n",
      "The loss after 93 epochs was 96.31404113769531\n",
      "The loss after 94 epochs was 96.04561614990234\n",
      "The loss after 95 epochs was 95.14068603515625\n",
      "The loss after 96 epochs was 96.40420532226562\n",
      "The loss after 97 epochs was 94.92510223388672\n",
      "The loss after 98 epochs was 95.15623474121094\n",
      "The loss after 99 epochs was 95.06946563720703\n"
     ]
    }
   ],
   "source": [
    "y_train = BostonModel()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "trainer = PyTorchTrainer(net, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "            epochs=100,\n",
    "            eval_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "mnist_trainset = MNIST(root=\"../exploratory/data/\", train=True, download=True, transform=None)\n",
    "mnist_testset = MNIST(root=\"../exploratory/data/\", train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist_trainset\n",
    "num_labels = len(data.train_labels)\n",
    "train_labels = torch.zeros(num_labels, 10)\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][data.train_labels[i]] = 1\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist_testset\n",
    "num_labels = len(data.test_labels)\n",
    "test_labels = torch.zeros(num_labels, 10)\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][data.test_labels[i]] = 1\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_trainset.train_data.type(torch.float32).unsqueeze(3) / 255.0\n",
    "mnist_test = mnist_testset.test_data.type(torch.float32).unsqueeze(3) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist_train\n",
    "X_test = mnist_test\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_perm = X_train.permute(0, 3, 1, 2)\n",
    "X_test_perm = X_test.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_ConvNet(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert_dim(x, 4)\n",
    "        \n",
    "        # num channgels\n",
    "        assert x.shape[1] == 1        \n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PyTorchTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train_perm, mnist_trainset.train_labels, \n",
    "            X_test_perm, mnist_testset.test_labels,\n",
    "            epochs=1,\n",
    "            eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(X_train_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a custom trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(X: Tensor, seed=1):\n",
    "    perm = torch.randperm(X.shape[0])\n",
    "    return X[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderTrainer(PyTorchTrainer):\n",
    "    def __init__(self,\n",
    "                 model: PyTorchModel,\n",
    "                 optim: Optimizer,\n",
    "                 criterion: _Loss):\n",
    "        super().__init__(model, optim, criterion)\n",
    "        self._check_optim_net_aligned()\n",
    "        \n",
    "    def _generate_batches(self,\n",
    "                          X: Tensor,\n",
    "                          size: int = 32) -> Tuple[Tensor]:\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch = X[ii:ii+size]\n",
    "\n",
    "            yield X_batch\n",
    "\n",
    "\n",
    "    def fit(self, X_train: Tensor,\n",
    "            X_test: Tensor,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32):\n",
    "\n",
    "        for e in range(epochs):\n",
    "            X_train = permute_data(X_train)\n",
    "\n",
    "            batch_generator = self._generate_batches(X_train, batch_size)\n",
    "\n",
    "            for ii, X_batch in enumerate(batch_generator):\n",
    "                \n",
    "                self.optim.zero_grad()   # zero the gradient buffers\n",
    "                encoding, output = self.model(X_batch)\n",
    "                loss = self.loss(output, X_batch)\n",
    "                loss.backward()\n",
    "                self.optim.step()    # Does the update\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            _, output = self.model(X_test)\n",
    "            loss = self.loss(output, X_test)\n",
    "            print(e, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model follows [here](https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderSplit(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d1 = nn.Conv2d(1, 16, 3, stride=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv2d2 = nn.Conv2d(16, 8, 3, stride=2, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, stride=1)\n",
    "        \n",
    "        self.conv2dT1 = nn.ConvTranspose2d(8, 16, 3, stride=2)\n",
    "        self.conv2dT2 = nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1)\n",
    "        self.conv2dT3 = nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1)  # b, 1, 28, 28\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2d2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        encoded = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.conv2dT1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2dT2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2dT3(x)\n",
    "        decoded = self.tanh(x)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model = AutoEncoderSplit()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoEncoderTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train_perm, X_train_perm, \n",
    "            epochs=1,\n",
    "            eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded, _ = model(X_test_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work! GANs could be done similarly. Will examine later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really do AE or GAN, you'll need to write custom trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working backwards:\n",
    "\n",
    "* Want a character level model - predict next char."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do it?\n",
    "\n",
    "Pass in sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New classes: `NextCharacterModel` and `LSTMTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 1, 5)\n",
    "print(a)\n",
    "a.repeat(1, 3, 1).shape\n",
    "a.repeat(1, 3, 1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextCharacterModel(PyTorchModel):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 neurons: int = 256,\n",
    "                 sequence_length: int = 25,\n",
    "                 reset_every: int = 100):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = neurons\n",
    "        self.lstm = nn.LSTM(vocab_size, neurons,\n",
    "                            batch_first=True)\n",
    "        self.fc_last = nn.Linear(neurons, vocab_size)\n",
    "        self.count = 0\n",
    "        self.reset_every = reset_every\n",
    "\n",
    "    def forward(self,\n",
    "                inputs: Tensor):\n",
    "        assert_dim(inputs, 3) # batch_size, sequence_length, vocab_size\n",
    "\n",
    "        if self.count % self.reset_every == 0:\n",
    "            self.hidden, self.cells = torch.randn(1, 1, self.hidden_size),\\\n",
    "                torch.randn(1, 1, self.hidden_size)\n",
    "        \n",
    "        self.count += 1\n",
    "        \n",
    "        hidden, cells = self.hidden.repeat(1, inputs.shape[0], 1),\\\n",
    "            self.cells.repeat(1, inputs.shape[0], 1)\n",
    "\n",
    "        out, (hidden_out, cells_out) = self.lstm(inputs, (hidden.data, cells.data))\n",
    "        \n",
    "        self.hidden.data, self.cells.data = hidden_out.data.mean(dim=1), cells_out.data.mean(dim=1)\n",
    "        \n",
    "        out = self.fc_last(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTrainer(PyTorchTrainer):\n",
    "    def __init__(self,\n",
    "                 model: NextCharacterModel,\n",
    "                 optim: Optimizer,\n",
    "                 criterion: _Loss):\n",
    "        super().__init__(model, optim, criterion)\n",
    "        self.vocab_size = self.model.vocab_size\n",
    "        self.max_len = self.model.sequence_length\n",
    "        \n",
    "    def fit(self,\n",
    "            data: str,\n",
    "            epochs: int=10,\n",
    "            eval_every: int=1,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 121718)-> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.train_data, self.test_data = self._train_test_split_text()\n",
    "        self.chars = list(set(self.data))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            batch_generator = self.generate_batches_next_char(batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                print(ii)\n",
    "#                 if ii == 1:\n",
    "#                     import pdb; pdb.set_trace()\n",
    "                self.optim.zero_grad()                \n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.loss(outputs, y_batch)\n",
    "                print(loss)\n",
    "                loss.backward()\n",
    "                self.optim.step()    # Does the update\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                X_test, y_test = self.generate_test_data()\n",
    "            \n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "\n",
    "    def _train_test_split_text(self, pct=0.8) -> Tuple[str]:\n",
    "\n",
    "        n = len(self.data)\n",
    "        return self.data[:int(n * pct)], self.data[int(n * pct):]\n",
    "\n",
    "    def generate_batches_next_char(self,\n",
    "                                   batch_size: int) -> Tuple[Tensor]:\n",
    "        N = len(self.train_data)\n",
    "        # add batch size\n",
    "        for ii in range(0, N, batch_size):\n",
    "\n",
    "            features_tensors = []\n",
    "            target_tensors = []\n",
    "\n",
    "            for char in range(batch_size):\n",
    "\n",
    "                features_str, target_str =\\\n",
    "                 self.train_data[ii+char:ii+char+self.max_len],\\\n",
    "                 self.train_data[ii+char+1:ii+char+self.max_len+1]\n",
    "\n",
    "                features_array, target_array =\\\n",
    "                    self._string_to_one_hot_array(features_str),\\\n",
    "                    self._string_to_one_hot_array(target_str)\n",
    "\n",
    "                features_tensors.append(features_array)\n",
    "                target_tensors.append(target_array)\n",
    "\n",
    "            yield torch.stack(features_tensors), torch.stack(target_tensors)\n",
    "\n",
    "    def _string_to_one_hot_array(self, input_string: str) -> Tuple[Tensor]:\n",
    "\n",
    "        ind = [self.char_to_idx[ch] for ch in input_string]\n",
    "\n",
    "        array = self._one_hot_text_data(ind)\n",
    "\n",
    "        return array\n",
    "\n",
    "    def _one_hot_text_data(self,\n",
    "                           sequence: List):\n",
    "\n",
    "        sequence_length = len(sequence)\n",
    "        batch = torch.zeros(sequence_length, self.vocab_size)\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "\n",
    "        return Tensor(batch)\n",
    "\n",
    "    def generate_test_data(self) -> Tuple[Tensor]:\n",
    "\n",
    "        features_str, target_str = self.test_data[:-1], self.test_data[1:]\n",
    "\n",
    "        X_tensors = []\n",
    "        y_tensors = []\n",
    "\n",
    "        N = len(self.test_data)\n",
    "\n",
    "        for start in range(0, N, self.max_len):\n",
    "\n",
    "            features_str, target_str =\\\n",
    "             self.test_data[start:start+self.max_len],\\\n",
    "             self.test_data[start+1:start+self.max_len+1]\n",
    "\n",
    "            features_array, target_array =\\\n",
    "                self._string_to_one_hot_array(features_str),\\\n",
    "                self._string_to_one_hot_array(target_str)\n",
    "\n",
    "            X_tensors.append(features_array)\n",
    "            y_tensors.append(target_array)\n",
    "\n",
    "        return torch.stack(X_tensors), torch.stack(y_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/input.txt', 'r').read()\n",
    "vocab_size = len(set(data))\n",
    "model = NextCharacterModel(vocab_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer = LSTMTrainer(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* Write code to generate next character from this.\n",
    "* Write early stopping code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section name: Grokking Advanced Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same data source as LSTM: \n",
    "\n",
    "* TODO: Draw computational graph\n",
    "* TODO: Describe input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Turing Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
