{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks:\n",
    "\n",
    "* PyTorch first pass\n",
    "    * `Model` with `forward` method.\n",
    "    * Manual training loop\n",
    "* PyTorch second pass\n",
    "    * `Model` with `forward` method.\n",
    "    * `Trainer` class that takes in:\n",
    "        * `Model`\n",
    "        * `Optimizer`\n",
    "        * `_Loss`\n",
    "\n",
    "Models:\n",
    "\n",
    "* Boston dataset (used for testing)\n",
    "* MNIST Conv net\n",
    "* LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import Tuple, List\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from lincoln.utils import permute_data, assert_dim\n",
    "\n",
    "from lincoln.pytorch.model import PyTorchModel\n",
    "from lincoln.pytorch.train import PyTorchTrainer\n",
    "from lincoln.pytorch.preprocessor import ConvNetPreprocessor\n",
    "\n",
    "torch.manual_seed(20190325);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Trainer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Boston model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchLayer(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "def inference_mode(m: nn.Module):\n",
    "    m.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(PyTorchLayer):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 neurons: int,\n",
    "                 dropout: float = 1.0,\n",
    "                 activation: nn.Module = None) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, neurons)\n",
    "        self.activation = activation\n",
    "        if dropout < 1.0:\n",
    "            self.dropout = nn.Dropout(1 - dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor,\n",
    "                inference: bool = False) -> Tensor:\n",
    "        if inference:\n",
    "            self.apply(inference_mode)\n",
    "        \n",
    "        x = self.linear(x) # does weight multiplication + bias\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(PyTorchModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = 13,\n",
    "                 hidden_dropout: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.dense1 = DenseLayer(13, hidden_size, \n",
    "                                 activation=nn.Tanh(),\n",
    "                                 dropout = hidden_dropout)\n",
    "        self.dense2 = DenseLayer(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: Tensor,\n",
    "                inference: bool = False) -> Tensor:\n",
    "        \n",
    "        assert_dim(x, 2)\n",
    "        \n",
    "        assert x.shape[1] == 13\n",
    "\n",
    "        x = self.dense1(x, inference)\n",
    "        return self.dense2(x, inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer, loss\n",
    "pytorch_boston_model = BostonModel(hidden_size=13, hidden_dropout=0.8)\n",
    "optimizer = optim.SGD(pytorch_boston_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d93f9816cded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             final_lr_exp = 0.001)\n\u001b[0m",
      "\u001b[0;32m~/development/lincoln/lincoln/pytorch/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_test, y_test, epochs, eval_every, batch_size, final_lr_exp)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# zero the gradient buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "trainer = PyTorchTrainer(pytorch_boston_model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "            epochs=100,\n",
    "            eval_every=10,\n",
    "            final_lr_exp = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.39127254486084"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.pow(pytorch_boston_model(X_test, inference=True) - y_test, 2)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pytorch_boston_model(X_test).view(-1)\n",
    "test_actual = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.detach().numpy()\n",
    "test_actual = test_actual.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a20fdbda0>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(test_pred, test_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "mnist_trainset = MNIST(root=\"../exploratory/data/\", train=True, download=True, transform=None)\n",
    "mnist_testset = MNIST(root=\"../exploratory/data/\", train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_trainset.train_data.type(torch.float32).unsqueeze(3) / 255.0\n",
    "mnist_test = mnist_testset.test_data.type(torch.float32).unsqueeze(3) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(PyTorchLayer):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 filter_size: int,\n",
    "                 activation: nn.Module = None,\n",
    "                 dropout: float = 1.0,\n",
    "                 flatten: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, filter_size, \n",
    "                              padding=filter_size // 2)\n",
    "        self.activation = activation\n",
    "        self.flatten = flatten\n",
    "        if dropout < 1.0:\n",
    "            self.dropout = nn.Dropout(1 - dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        x = self.conv(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            x = self.dropout(x)            \n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_ConvNet(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvLayer(1, 16, 5, activation=nn.Tanh(),\n",
    "                               dropout=0.8)\n",
    "        self.conv2 = ConvLayer(16, 8, 5, activation=nn.Tanh(), flatten=True,\n",
    "                               dropout=0.8)\n",
    "        self.dense1 = DenseLayer(28 * 28 * 8, 32, activation=nn.Tanh(),\n",
    "                                 dropout=0.8)\n",
    "        self.dense2 = DenseLayer(32, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        assert_dim(x, 4)\n",
    "            \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = mnist_train.permute(0, 3, 1, 2), mnist_test.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1721)\n",
      "1 tensor(0.1088)\n",
      "2 tensor(0.0924)\n",
      "3 tensor(0.0881)\n",
      "4 tensor(0.0832)\n"
     ]
    }
   ],
   "source": [
    "trainer = PyTorchTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, mnist_trainset.train_labels, \n",
    "            X_test, mnist_testset.test_labels,\n",
    "            epochs=5,\n",
    "            eval_every=1,\n",
    "            final_lr_exp = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9815)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model(X_test)\n",
    "(torch.max(out, dim=1)[1] == mnist_testset.test_labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9747)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "out = model(X_test)\n",
    "(torch.max(out, dim=1)[1] == mnist_testset.test_labels).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~98% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working backwards:\n",
    "\n",
    "* Want a character level model - predict next char."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do it?\n",
    "\n",
    "Pass in sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New classes: `NextCharacterModel` and `LSTMTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5 = seq_len\n",
    "* 3 = batch\n",
    "* 10 = input_size\n",
    "* 20 = hidden_size\n",
    "\n",
    "* h0 = `(num_layers, batch_size, hidden_size)` = (2, 3, 20)\n",
    "\n",
    "nn.LSTM(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(PyTorchLayer):\n",
    "    def __init__(self,\n",
    "                 sequence_length: int,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 dropout: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_init = torch.zeros((1, hidden_size))\n",
    "        self.c_init = torch.zeros((1, hidden_size))\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = DenseLayer(hidden_size, output_size)\n",
    "        if dropout < 1.0:\n",
    "            self.dropout = nn.Dropout(1 - dropout)\n",
    "\n",
    "        \n",
    "    def _transform_hidden_batch(self, hidden: Tensor,\n",
    "                                batch_size: int,\n",
    "                                before_layer: bool) -> Tensor:\n",
    "        \n",
    "        if before_layer:\n",
    "            return (hidden\n",
    "                    .repeat(batch_size, 1)\n",
    "                    .view(batch_size, 1, self.hidden_size)\n",
    "                    .permute(1,0,2))\n",
    "        else:\n",
    "            return (hidden\n",
    "                    .permute(1,0,2)\n",
    "                    .mean(dim=0))         \n",
    "    \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h_layer = self._transform_hidden_batch(self.h_init, batch_size, before_layer=True)\n",
    "        c_layer = self._transform_hidden_batch(self.c_init, batch_size, before_layer=True)\n",
    "        \n",
    "        x, (h_out, c_out) = self.lstm(x, (h_layer, c_layer))\n",
    "        \n",
    "        self.h_init, self.c_init = (\n",
    "            self._transform_hidden_batch(h_out, batch_size, before_layer=False).detach(),\n",
    "            self._transform_hidden_batch(c_out, batch_size, before_layer=False).detach()\n",
    "        )\n",
    "\n",
    "        x = self.fc(x)\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            x = self.dropout(x) \n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay = LSTMLayer(sequence_length=25,\n",
    "          input_size=62,\n",
    "          hidden_size=100,\n",
    "          output_size=128)\n",
    "\n",
    "x = torch.randn(32, 25, 62)\n",
    "\n",
    "lay(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextCharacterModel(PyTorchModel):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 hidden_size: int = 256,\n",
    "                 sequence_length: int = 25):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # In this model, we have only one layer, with the same output size as input_size\n",
    "        self.lstm = LSTMLayer(self.sequence_length, self.vocab_size, hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                inputs: Tensor):\n",
    "        assert_dim(inputs, 3) # batch_size, sequence_length, vocab_size\n",
    "\n",
    "        out = self.lstm(inputs)       \n",
    "        \n",
    "        return out.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTrainer(PyTorchTrainer):\n",
    "    def __init__(self,\n",
    "                 model: NextCharacterModel,\n",
    "                 optim: Optimizer,\n",
    "                 criterion: _Loss):\n",
    "        super().__init__(model, optim, criterion)\n",
    "        self.vocab_size = self.model.vocab_size\n",
    "        self.max_len = self.model.sequence_length\n",
    "        \n",
    "    def fit(self,\n",
    "            data: str,\n",
    "            epochs: int=10,\n",
    "            eval_every: int=1,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 121718)-> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.train_data, self.test_data = self._train_test_split_text()\n",
    "        self.chars = list(set(self.data))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        losses = deque(maxlen=50)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "\n",
    "            batch_generator = self.generate_batches_next_char(batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.optim.zero_grad()                \n",
    "                outputs = self.model(X_batch)\n",
    "\n",
    "                loss = self.loss(outputs, y_batch)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                print(loss.item())\n",
    "                \n",
    "                self.optim.step()    \n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                X_test, y_test = self.generate_test_data()\n",
    "            \n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "\n",
    "    def _train_test_split_text(self, pct=0.8) -> Tuple[str]:\n",
    "\n",
    "        n = len(self.data)\n",
    "        return self.data[:int(n * pct)], self.data[int(n * pct):]\n",
    "\n",
    "    def generate_batches_next_char(self,\n",
    "                                   batch_size: int) -> Tuple[Tensor]:\n",
    "        N = len(self.train_data)\n",
    "        # add batch size\n",
    "        for ii in range(0, N, batch_size):\n",
    "\n",
    "            features_tensors = []\n",
    "            target_indices = []\n",
    "\n",
    "            for char in range(batch_size):\n",
    "\n",
    "                features_str, target_str =\\\n",
    "                 self.train_data[ii+char:ii+char+self.max_len],\\\n",
    "                 self.train_data[ii+char+1:ii+char+self.max_len+1]\n",
    "\n",
    "                features_array = self._string_to_one_hot_array(features_str)\n",
    "                target_indices_seq = [self.char_to_idx[char] for char in target_str]\n",
    "\n",
    "                features_tensors.append(features_array)\n",
    "                target_indices.append(target_indices_seq)\n",
    "#             import pdb; pdb.set_trace()\n",
    "            yield torch.stack(features_tensors), torch.LongTensor(target_indices)\n",
    "\n",
    "    def _string_to_one_hot_array(self, input_string: str) -> Tuple[Tensor]:\n",
    "\n",
    "        ind = [self.char_to_idx[ch] for ch in input_string]\n",
    "\n",
    "        array = self._one_hot_text_data(ind)\n",
    "\n",
    "        return array\n",
    "\n",
    "    def _one_hot_text_data(self,\n",
    "                           sequence: List):\n",
    "\n",
    "        sequence_length = len(sequence)\n",
    "        batch = torch.zeros(sequence_length, self.vocab_size)\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "\n",
    "        return Tensor(batch)\n",
    "\n",
    "    def generate_test_data(self) -> Tuple[Tensor]:\n",
    "\n",
    "        features_str, target_str = self.test_data[:-1], self.test_data[1:]\n",
    "\n",
    "        X_tensors = []\n",
    "        y_tensors = []\n",
    "\n",
    "        N = len(self.test_data)\n",
    "\n",
    "        for start in range(0, N, self.max_len):\n",
    "\n",
    "            features_str, target_str =\\\n",
    "             self.test_data[start:start+self.max_len],\\\n",
    "             self.test_data[start+1:start+self.max_len+1]\n",
    "\n",
    "            features_array, target_array =\\\n",
    "                self._string_to_one_hot_array(features_str),\\\n",
    "                self._string_to_one_hot_array(target_str)\n",
    "\n",
    "            X_tensors.append(features_array)\n",
    "            y_tensors.append(target_array)\n",
    "\n",
    "        return torch.stack(X_tensors), torch.stack(y_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/input.txt', 'r').read()\n",
    "vocab_size = len(set(data))\n",
    "model = NextCharacterModel(vocab_size, hidden_size=vocab_size, sequence_length=50)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer = LSTMTrainer(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.137842178344727\n",
      "4.104428291320801\n",
      "4.086086750030518\n",
      "4.090394020080566\n",
      "4.0924482345581055\n",
      "4.089087009429932\n",
      "4.073104381561279\n",
      "4.051155090332031\n",
      "4.05943489074707\n",
      "4.077125549316406\n",
      "4.063826084136963\n",
      "4.062566757202148\n",
      "4.043614387512207\n",
      "3.9929184913635254\n",
      "4.002445697784424\n",
      "4.00609016418457\n",
      "3.979388952255249\n",
      "3.9607019424438477\n",
      "3.946185350418091\n",
      "3.942286729812622\n",
      "3.9354352951049805\n",
      "3.956104040145874\n",
      "3.8561203479766846\n",
      "3.790559768676758\n",
      "3.7548093795776367\n",
      "3.7242326736450195\n",
      "3.6455228328704834\n",
      "3.5629355907440186\n",
      "3.5222766399383545\n",
      "3.4115805625915527\n",
      "3.3349039554595947\n",
      "3.516964912414551\n",
      "3.6808338165283203\n",
      "3.256007432937622\n",
      "3.263782024383545\n",
      "3.1392531394958496\n",
      "3.1838948726654053\n",
      "3.272329807281494\n",
      "3.1563568115234375\n",
      "3.095775842666626\n",
      "3.3259458541870117\n",
      "3.2612555027008057\n",
      "3.1526522636413574\n",
      "3.168083429336548\n",
      "3.0873849391937256\n",
      "2.9401650428771973\n",
      "2.8889834880828857\n",
      "3.133181571960449\n",
      "3.1204183101654053\n",
      "3.0676934719085693\n",
      "3.0140979290008545\n",
      "3.2145941257476807\n",
      "4.004598617553711\n",
      "3.425161838531494\n",
      "3.11283540725708\n",
      "3.058790683746338\n",
      "3.042586326599121\n",
      "3.025796890258789\n",
      "2.9831113815307617\n",
      "3.023277521133423\n",
      "3.1646785736083984\n",
      "3.79677677154541\n",
      "3.6255545616149902\n",
      "4.135970592498779\n",
      "4.4524126052856445\n",
      "3.6365652084350586\n",
      "3.1569223403930664\n",
      "2.968658208847046\n",
      "2.911057472229004\n",
      "3.02681827545166\n",
      "3.3367702960968018\n",
      "3.3647665977478027\n",
      "3.059567451477051\n",
      "3.120436429977417\n",
      "3.2151906490325928\n",
      "3.242213487625122\n",
      "3.518899917602539\n",
      "3.631577730178833\n",
      "3.7234232425689697\n",
      "3.591324806213379\n",
      "3.729588270187378\n",
      "3.261049747467041\n",
      "3.3740615844726562\n",
      "3.4957120418548584\n",
      "4.252573013305664\n",
      "3.7624335289001465\n",
      "3.358621597290039\n",
      "3.2329463958740234\n",
      "3.31034779548645\n",
      "3.1852078437805176\n",
      "3.227390766143799\n",
      "3.1324048042297363\n",
      "3.3578102588653564\n",
      "3.5091969966888428\n",
      "3.2078697681427\n",
      "3.2570290565490723\n",
      "3.6368792057037354\n",
      "3.6439292430877686\n",
      "3.055776357650757\n",
      "3.334519386291504\n",
      "3.6220779418945312\n",
      "3.1617417335510254\n",
      "3.215693950653076\n",
      "3.2946813106536865\n",
      "3.3465981483459473\n",
      "3.2809317111968994\n",
      "3.1899521350860596\n",
      "3.6635422706604004\n",
      "3.3839621543884277\n",
      "3.1581475734710693\n",
      "3.1469204425811768\n",
      "3.1983933448791504\n",
      "3.1749377250671387\n",
      "3.780517816543579\n",
      "3.8216564655303955\n",
      "3.529179096221924\n",
      "3.6540143489837646\n",
      "3.380995273590088\n",
      "3.141291618347168\n",
      "3.1079397201538086\n",
      "3.074491500854492\n",
      "3.1589417457580566\n",
      "3.0450077056884766\n",
      "3.3554847240448\n",
      "3.387605905532837\n",
      "3.4595301151275635\n",
      "3.6111137866973877\n",
      "3.2904903888702393\n",
      "3.248399257659912\n",
      "3.392721652984619\n",
      "3.6066176891326904\n",
      "3.4063241481781006\n",
      "3.2371666431427\n",
      "3.2738254070281982\n",
      "3.160341739654541\n",
      "3.312608003616333\n",
      "3.2013161182403564\n",
      "3.1819205284118652\n",
      "3.210521936416626\n",
      "3.259000778198242\n",
      "3.4335083961486816\n",
      "3.1100986003875732\n",
      "3.0850179195404053\n",
      "3.0861597061157227\n",
      "3.135648488998413\n",
      "3.286247491836548\n",
      "3.3455517292022705\n",
      "3.2207834720611572\n",
      "3.1897361278533936\n",
      "3.1170196533203125\n",
      "3.3724570274353027\n",
      "3.5215718746185303\n",
      "3.066774845123291\n",
      "3.0729455947875977\n",
      "3.0818495750427246\n",
      "3.1126351356506348\n",
      "3.5440421104431152\n",
      "3.632251262664795\n",
      "3.694000482559204\n",
      "3.683105707168579\n",
      "3.2695014476776123\n",
      "3.094326972961426\n",
      "3.059271812438965\n",
      "3.0712859630584717\n",
      "3.058084726333618\n",
      "3.173941135406494\n",
      "3.6214709281921387\n",
      "3.3046371936798096\n",
      "3.143578290939331\n",
      "3.150632619857788\n",
      "3.0726020336151123\n",
      "2.9853670597076416\n",
      "3.37086820602417\n",
      "3.4484200477600098\n",
      "3.0833580493927\n",
      "3.1127512454986572\n",
      "3.5246634483337402\n",
      "3.3054566383361816\n",
      "3.0070254802703857\n",
      "3.4974913597106934\n",
      "3.7980048656463623\n",
      "3.537815809249878\n",
      "3.149552822113037\n",
      "3.290688991546631\n",
      "3.214036464691162\n",
      "3.004007577896118\n",
      "3.1077826023101807\n",
      "3.4416136741638184\n",
      "3.3386855125427246\n",
      "3.1164069175720215\n",
      "3.0638184547424316\n",
      "2.9929816722869873\n",
      "3.1062684059143066\n",
      "3.5628232955932617\n",
      "3.6838278770446777\n",
      "3.1008124351501465\n",
      "3.556046485900879\n",
      "3.4822728633880615\n",
      "3.2541823387145996\n",
      "3.255690574645996\n",
      "3.162175178527832\n",
      "3.438878059387207\n",
      "3.6859564781188965\n",
      "3.625300407409668\n",
      "3.2527120113372803\n",
      "3.225536584854126\n",
      "3.06294322013855\n",
      "3.221737861633301\n",
      "3.680466651916504\n",
      "3.6679487228393555\n",
      "3.2867591381073\n",
      "3.117398977279663\n",
      "3.108901262283325\n",
      "3.2114455699920654\n",
      "3.1265366077423096\n",
      "3.1651389598846436\n",
      "3.292891025543213\n",
      "3.266057014465332\n",
      "3.138700246810913\n",
      "3.2230379581451416\n",
      "3.212336778640747\n",
      "3.2003369331359863\n",
      "3.0684328079223633\n",
      "3.0192344188690186\n",
      "3.064913272857666\n",
      "3.1482467651367188\n",
      "3.626204490661621\n",
      "3.415808916091919\n",
      "3.5690276622772217\n",
      "3.916729211807251\n",
      "3.659456491470337\n",
      "3.714552640914917\n",
      "3.482365369796753\n",
      "3.1138319969177246\n",
      "3.092877149581909\n",
      "3.1340157985687256\n",
      "3.474050283432007\n",
      "3.2959461212158203\n",
      "3.2681691646575928\n",
      "3.182285785675049\n",
      "3.2081527709960938\n",
      "3.1506903171539307\n",
      "3.2132978439331055\n",
      "3.2667293548583984\n",
      "3.327589988708496\n",
      "3.440680503845215\n",
      "3.167607069015503\n",
      "3.1111135482788086\n",
      "3.184462308883667\n",
      "3.1791741847991943\n",
      "3.44287109375\n",
      "3.268667697906494\n",
      "3.1713576316833496\n",
      "3.509115219116211\n",
      "3.255714178085327\n",
      "3.1925876140594482\n",
      "3.1726293563842773\n",
      "3.2040820121765137\n",
      "3.2605342864990234\n",
      "3.1662631034851074\n",
      "3.0569446086883545\n",
      "2.9834182262420654\n",
      "2.9807372093200684\n",
      "2.925285577774048\n",
      "2.9554946422576904\n",
      "3.04933500289917\n",
      "3.0665879249572754\n",
      "3.049215316772461\n",
      "3.0955333709716797\n",
      "3.3875153064727783\n",
      "3.6626996994018555\n",
      "3.1083037853240967\n",
      "3.024038314819336\n",
      "2.9749536514282227\n",
      "3.0871171951293945\n",
      "2.9338107109069824\n",
      "3.1191296577453613\n",
      "3.9778151512145996\n",
      "3.432657241821289\n",
      "2.926025629043579\n",
      "2.8715322017669678\n",
      "2.9994475841522217\n",
      "3.0713491439819336\n",
      "2.980624198913574\n",
      "3.3480308055877686\n",
      "3.570460796356201\n",
      "3.2815518379211426\n",
      "3.3840341567993164\n",
      "3.1472787857055664\n",
      "2.889875888824463\n",
      "2.9015839099884033\n",
      "3.0268051624298096\n",
      "3.0982189178466797\n",
      "3.0891077518463135\n",
      "2.9786765575408936\n",
      "3.1280248165130615\n",
      "3.199866533279419\n",
      "3.0223848819732666\n",
      "3.0773355960845947\n",
      "3.001721143722534\n",
      "2.908108711242676\n",
      "3.0051283836364746\n",
      "3.285097360610962\n",
      "3.0324578285217285\n",
      "2.8993723392486572\n",
      "2.9201269149780273\n",
      "2.9391281604766846\n",
      "3.1642026901245117\n",
      "3.1063055992126465\n",
      "2.9897539615631104\n",
      "3.1466360092163086\n",
      "3.0314817428588867\n",
      "3.0517313480377197\n",
      "2.9657514095306396\n",
      "2.920874834060669\n",
      "3.4802968502044678\n",
      "3.247873306274414\n",
      "3.2642717361450195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b07782b6ec45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-43721ef0ea6e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, epochs, eval_every, batch_size, seed)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-e38ff0fef99f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0massert_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch_size, sequence_length, vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-969260095600>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mc_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_hidden_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         self.h_init, self.c_init = (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_trainer.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* Write code to generate next character from this.\n",
    "* Write early stopping code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_ConvNet(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvLayer(1, 14, 5, activation=nn.Tanh(),\n",
    "                               dropout=0.8)\n",
    "        self.conv2 = ConvLayer(14, 7, 5, activation=nn.Tanh(), flatten=True,\n",
    "                               dropout=0.8)\n",
    "        self.dense1 = DenseLayer(28 * 28 * 7, 32, activation=nn.Tanh(),\n",
    "                                 dropout=0.8)\n",
    "        self.dense2 = DenseLayer(32, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        assert_dim(x, 4)\n",
    "            \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        last_dense = self.dense1(x)\n",
    "        final_prediction = self.dense2(last_dense)\n",
    "        return final_prediction, last_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = PyTorchTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, mnist_trainset.train_labels, \n",
    "            X_test, mnist_testset.test_labels,\n",
    "            epochs=5,\n",
    "            eval_every=1,\n",
    "            final_lr_exp = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvLayer(PyTorchLayer):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 filter_size: int,\n",
    "                 activation: nn.Module = None,\n",
    "                 dropout: float = 1.0,\n",
    "                 flatten: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, filter_size, \n",
    "                                       padding=filter_size // 2)\n",
    "        self.activation = activation\n",
    "        self.flatten = flatten\n",
    "        if dropout < 1.0:\n",
    "            self.dropout = nn.Dropout(1 - dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        x = self.deconv(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            x = self.dropout(x)            \n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.conv1 = ConvLayer(1, 14, 5, activation=nn.Tanh())\n",
    "        self.conv2 = ConvLayer(14, 7, 5, activation=nn.Tanh(), flatten=True)\n",
    "        \n",
    "        self.dense1 = DenseLayer(7 * 28 * 28, 3, activation=nn.Tanh())\n",
    "        self.dense2 = DenseLayer(3, 7 * 28 * 28, activation=nn.Tanh())\n",
    "        \n",
    "        self.conv3 = DeconvLayer(7, 14, 5, activation=nn.Tanh()) \n",
    "        self.conv4 = DeconvLayer(14, 1, 5, activation=nn.Sigmoid())         \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        assert_dim(x, 4)\n",
    "            \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        encoding = self.dense1(x)\n",
    "        \n",
    "        x = self.dense2(encoding)\n",
    "        \n",
    "        x = x.view(-1, 7, 28, 28)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        return x, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 0 epochs was 0.06720604002475739\n",
      "The loss after 1 epochs was 0.06217539310455322\n",
      "The loss after 2 epochs was 0.060440342873334885\n",
      "The loss after 3 epochs was 0.05955014377832413\n",
      "The loss after 4 epochs was 0.059032004326581955\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "trainer = PyTorchTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, X_train, \n",
    "            X_test, X_test,\n",
    "            epochs=5,\n",
    "            eval_every=1,\n",
    "            batch_size=60,\n",
    "            final_lr_exp = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=10, random_state=20190405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_result = TSNE(n_components=2).fit_transform(out[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-392-2e9da45efd8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtsne_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    768\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_iter_without_progress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 827\u001b[0;31m                                                           **opt_args)\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# Save the final number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[1;32m    246\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                                       dof=degrees_of_freedom)\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "tsne_df = pd.DataFrame({'tsne_dim_1': tsne_result[:,0], \n",
    "              'tsne_dim_2': tsne_result[:,1],\n",
    "              'category': mnist_testset.test_labels})\n",
    "\n",
    "tsne_df.plot.scatter(x='tsne_dim_1', y='tsne_dim_2')\n",
    "\n",
    "groups = tsne_df.groupby('category')\n",
    "for group in groups:\n",
    "    print(group[0])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(25,25))\n",
    "ax.mairgins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.scatter(group['tsne_dim_1'], group['tsne_dim_2'], marker='o', label=name)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
