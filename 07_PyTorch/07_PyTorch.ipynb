{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks:\n",
    "\n",
    "* PyTorch first pass\n",
    "    * `Model` with `forward` method.\n",
    "    * Manual training loop\n",
    "* PyTorch second pass\n",
    "    * `Model` with `forward` method.\n",
    "    * `Trainer` class that takes in:\n",
    "        * `Model`\n",
    "        * `Optimizer`\n",
    "        * `_Loss`\n",
    "\n",
    "Models:\n",
    "\n",
    "* Boston dataset (used for testing)\n",
    "* MNIST Conv net\n",
    "* LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import Tuple, List\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from lincoln.utils import permute_data, assert_dim\n",
    "\n",
    "from lincoln.pytorch.model import PyTorchModel\n",
    "from lincoln.pytorch.train import PyTorchTrainer\n",
    "from lincoln.pytorch.preprocessor import ConvNetPreprocessor\n",
    "\n",
    "torch.manual_seed(20190325);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Trainer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Boston model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchLayer(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(PyTorchLayer):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 neurons: int,\n",
    "                 activation: nn.Module = None) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        x = self.linear(x) # does weight multiplication + bias\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(PyTorchModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = 13):\n",
    "        super().__init__()\n",
    "        self.dense1 = DenseLayer(13, hidden_size, activation=nn.Tanh())\n",
    "        self.dense2 = DenseLayer(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        assert_dim(x, 2)\n",
    "        \n",
    "        assert x.shape[1] == 13\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer, loss\n",
    "pytorch_boston_model = BostonModel(hidden_size=13)\n",
    "optimizer = optim.SGD(pytorch_boston_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(552.8856)\n",
      "10 tensor(76.9690)\n",
      "20 tensor(46.1771)\n",
      "30 tensor(39.2791)\n",
      "40 tensor(33.5310)\n",
      "50 tensor(29.2469)\n",
      "60 tensor(25.4051)\n",
      "70 tensor(22.8295)\n",
      "80 tensor(20.7839)\n",
      "90 tensor(19.1065)\n",
      "100 tensor(20.2159)\n",
      "110 tensor(17.1996)\n",
      "120 tensor(15.6069)\n",
      "130 tensor(17.7104)\n",
      "140 tensor(14.1601)\n",
      "150 tensor(14.4325)\n",
      "160 tensor(12.9108)\n",
      "170 tensor(12.5565)\n",
      "180 tensor(12.7575)\n",
      "190 tensor(12.1290)\n",
      "200 tensor(12.6223)\n",
      "210 tensor(12.0121)\n",
      "220 tensor(12.2005)\n",
      "230 tensor(13.8037)\n",
      "240 tensor(11.8467)\n",
      "250 tensor(11.7453)\n",
      "260 tensor(11.6440)\n",
      "270 tensor(13.1057)\n",
      "280 tensor(12.6793)\n",
      "290 tensor(11.6068)\n"
     ]
    }
   ],
   "source": [
    "trainer = PyTorchTrainer(pytorch_boston_model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "            epochs=300,\n",
    "            eval_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.557090759277344"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.pow(pytorch_boston_model(X_test) - y_test, 2)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pytorch_boston_model(X_test).view(-1)\n",
    "test_actual = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.detach().numpy()\n",
    "test_actual = test_actual.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a29bceb70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(test_pred, test_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "mnist_trainset = MNIST(root=\"../exploratory/data/\", train=True, download=True, transform=None)\n",
    "mnist_testset = MNIST(root=\"../exploratory/data/\", train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_trainset.train_data.type(torch.float32).unsqueeze(3) / 255.0\n",
    "mnist_test = mnist_testset.test_data.type(torch.float32).unsqueeze(3) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(PyTorchLayer):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 filter_size: int,\n",
    "                 activation: nn.Module = None,\n",
    "                 flatten: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, filter_size, \n",
    "                              padding=filter_size // 2)\n",
    "        self.activation = activation\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        x = self.conv(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_ConvNet(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvLayer(1, 16, 5, activation=nn.Tanh())\n",
    "        self.conv2 = ConvLayer(16, 8, 5, activation=nn.Tanh(), flatten=True)\n",
    "        self.dense1 = DenseLayer(28 * 28 * 8, 32, activation=nn.Tanh())\n",
    "        self.dense2 = DenseLayer(32, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        assert_dim(x, 4)\n",
    "            \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "preprocessor = ConvNetPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = mnist_train.permute(0, 3, 1, 2), mnist_test.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0829)\n",
      "1 tensor(0.0683)\n",
      "2 tensor(0.0618)\n",
      "3 tensor(0.0560)\n",
      "4 tensor(0.0545)\n"
     ]
    }
   ],
   "source": [
    "trainer = PyTorchTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, mnist_trainset.train_labels, \n",
    "            X_test, mnist_testset.test_labels,\n",
    "            epochs=5,\n",
    "            eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9845)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.max(out, dim=1)[1] == mnist_testset.test_labels).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~98% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working backwards:\n",
    "\n",
    "* Want a character level model - predict next char."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do it?\n",
    "\n",
    "Pass in sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New classes: `NextCharacterModel` and `LSTMTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5 = seq_len\n",
    "* 3 = batch\n",
    "* 10 = input_size\n",
    "* 20 = hidden_size\n",
    "\n",
    "* h0 = `(num_layers, batch_size, hidden_size)` = (2, 3, 20)\n",
    "\n",
    "nn.LSTM(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(PyTorchLayer):\n",
    "    def __init__(self,\n",
    "                 sequence_length: int,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_init = torch.zeros((1, hidden_size))\n",
    "        self.c_init = torch.zeros((1, hidden_size))\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = DenseLayer(hidden_size, output_size)\n",
    "\n",
    "        \n",
    "    def _transform_hidden_batch(self, hidden: Tensor,\n",
    "                                batch_size: int,\n",
    "                                before_layer: bool) -> Tensor:\n",
    "        \n",
    "        if before_layer:\n",
    "            return (hidden\n",
    "                    .repeat(batch_size, 1)\n",
    "                    .view(batch_size, 1, self.hidden_size)\n",
    "                    .permute(1,0,2))\n",
    "        else:\n",
    "            return (hidden\n",
    "                    .permute(1,0,2)\n",
    "                    .mean(dim=0))         \n",
    "    \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h_layer = self._transform_hidden_batch(self.h_init, batch_size, before_layer=True)\n",
    "        c_layer = self._transform_hidden_batch(self.c_init, batch_size, before_layer=True)\n",
    "        \n",
    "        x, (h_out, c_out) = self.lstm(x, (h_layer, c_layer))\n",
    "        \n",
    "        self.h_init, self.c_init = (\n",
    "            self._transform_hidden_batch(h_out, batch_size, before_layer=False).detach(),\n",
    "            self._transform_hidden_batch(c_out, batch_size, before_layer=False).detach()\n",
    "        )\n",
    "\n",
    "        x = self.fc(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25, 128])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay = LSTMLayer(sequence_length=25,\n",
    "          input_size=62,\n",
    "          hidden_size=100,\n",
    "          output_size=128)\n",
    "\n",
    "x = torch.randn(32, 25, 62)\n",
    "\n",
    "lay(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextCharacterModel(PyTorchModel):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 hidden_size: int = 256,\n",
    "                 sequence_length: int = 25):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # In this model, we have only one layer, with the same output size as input_size\n",
    "        self.lstm = LSTMLayer(self.sequence_length, self.vocab_size, hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                inputs: Tensor):\n",
    "        assert_dim(inputs, 3) # batch_size, sequence_length, vocab_size\n",
    "\n",
    "        out = self.lstm(inputs)       \n",
    "        \n",
    "        return out.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTrainer(PyTorchTrainer):\n",
    "    def __init__(self,\n",
    "                 model: NextCharacterModel,\n",
    "                 optim: Optimizer,\n",
    "                 criterion: _Loss):\n",
    "        super().__init__(model, optim, criterion)\n",
    "        self.vocab_size = self.model.vocab_size\n",
    "        self.max_len = self.model.sequence_length\n",
    "        \n",
    "    def fit(self,\n",
    "            data: str,\n",
    "            epochs: int=10,\n",
    "            eval_every: int=1,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 121718)-> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.train_data, self.test_data = self._train_test_split_text()\n",
    "        self.chars = list(set(self.data))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        losses = deque(maxlen=50)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "\n",
    "            batch_generator = self.generate_batches_next_char(batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.optim.zero_grad()                \n",
    "                outputs = self.model(X_batch)\n",
    "\n",
    "                loss = self.loss(outputs, y_batch)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                print(loss.item())\n",
    "                \n",
    "                self.optim.step()    \n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                X_test, y_test = self.generate_test_data()\n",
    "            \n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "\n",
    "    def _train_test_split_text(self, pct=0.8) -> Tuple[str]:\n",
    "\n",
    "        n = len(self.data)\n",
    "        return self.data[:int(n * pct)], self.data[int(n * pct):]\n",
    "\n",
    "    def generate_batches_next_char(self,\n",
    "                                   batch_size: int) -> Tuple[Tensor]:\n",
    "        N = len(self.train_data)\n",
    "        # add batch size\n",
    "        for ii in range(0, N, batch_size):\n",
    "\n",
    "            features_tensors = []\n",
    "            target_indices = []\n",
    "\n",
    "            for char in range(batch_size):\n",
    "\n",
    "                features_str, target_str =\\\n",
    "                 self.train_data[ii+char:ii+char+self.max_len],\\\n",
    "                 self.train_data[ii+char+1:ii+char+self.max_len+1]\n",
    "\n",
    "                features_array = self._string_to_one_hot_array(features_str)\n",
    "                target_indices_seq = [self.char_to_idx[char] for char in target_str]\n",
    "\n",
    "                features_tensors.append(features_array)\n",
    "                target_indices.append(target_indices_seq)\n",
    "#             import pdb; pdb.set_trace()\n",
    "            yield torch.stack(features_tensors), torch.LongTensor(target_indices)\n",
    "\n",
    "    def _string_to_one_hot_array(self, input_string: str) -> Tuple[Tensor]:\n",
    "\n",
    "        ind = [self.char_to_idx[ch] for ch in input_string]\n",
    "\n",
    "        array = self._one_hot_text_data(ind)\n",
    "\n",
    "        return array\n",
    "\n",
    "    def _one_hot_text_data(self,\n",
    "                           sequence: List):\n",
    "\n",
    "        sequence_length = len(sequence)\n",
    "        batch = torch.zeros(sequence_length, self.vocab_size)\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "\n",
    "        return Tensor(batch)\n",
    "\n",
    "    def generate_test_data(self) -> Tuple[Tensor]:\n",
    "\n",
    "        features_str, target_str = self.test_data[:-1], self.test_data[1:]\n",
    "\n",
    "        X_tensors = []\n",
    "        y_tensors = []\n",
    "\n",
    "        N = len(self.test_data)\n",
    "\n",
    "        for start in range(0, N, self.max_len):\n",
    "\n",
    "            features_str, target_str =\\\n",
    "             self.test_data[start:start+self.max_len],\\\n",
    "             self.test_data[start+1:start+self.max_len+1]\n",
    "\n",
    "            features_array, target_array =\\\n",
    "                self._string_to_one_hot_array(features_str),\\\n",
    "                self._string_to_one_hot_array(target_str)\n",
    "\n",
    "            X_tensors.append(features_array)\n",
    "            y_tensors.append(target_array)\n",
    "\n",
    "        return torch.stack(X_tensors), torch.stack(y_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/input.txt', 'r').read()\n",
    "vocab_size = len(set(data))\n",
    "model = NextCharacterModel(vocab_size, hidden_size=vocab_size, sequence_length=50)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer = LSTMTrainer(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.127601146697998\n",
      "4.111598014831543\n",
      "4.105199337005615\n",
      "4.106496810913086\n",
      "4.0934858322143555\n",
      "4.077452182769775\n",
      "4.0833940505981445\n",
      "4.075546741485596\n",
      "4.0595383644104\n",
      "4.075404167175293\n",
      "4.078780651092529\n",
      "4.068609714508057\n",
      "4.054238319396973\n",
      "4.014968395233154\n",
      "4.008720874786377\n",
      "4.0064191818237305\n",
      "3.993131637573242\n",
      "3.9759786128997803\n",
      "3.959608554840088\n",
      "3.9405219554901123\n",
      "3.9246468544006348\n",
      "3.938478469848633\n",
      "3.869570732116699\n",
      "3.8072006702423096\n",
      "3.747617244720459\n",
      "3.732602119445801\n",
      "3.6944665908813477\n",
      "3.6197762489318848\n",
      "3.566596269607544\n",
      "3.4648680686950684\n",
      "3.3709990978240967\n",
      "3.5467846393585205\n",
      "3.693753242492676\n",
      "3.2585761547088623\n",
      "3.214423418045044\n",
      "3.1433823108673096\n",
      "3.181027412414551\n",
      "3.2272729873657227\n",
      "3.169543981552124\n",
      "3.0808281898498535\n",
      "3.2792317867279053\n",
      "3.2352590560913086\n",
      "3.123706102371216\n",
      "3.119932174682617\n",
      "3.0426392555236816\n",
      "2.914970636367798\n",
      "2.8726184368133545\n",
      "3.117824077606201\n",
      "3.114644765853882\n",
      "3.061026096343994\n",
      "3.00258207321167\n",
      "3.2040374279022217\n",
      "4.063312530517578\n",
      "3.4589874744415283\n",
      "3.0977673530578613\n",
      "3.0275888442993164\n",
      "3.0293009281158447\n",
      "3.0285844802856445\n",
      "2.9702541828155518\n",
      "3.035520553588867\n",
      "3.1818044185638428\n",
      "3.7752366065979004\n",
      "3.606511116027832\n",
      "4.132007122039795\n",
      "4.454963684082031\n",
      "3.6539511680603027\n",
      "3.1588611602783203\n",
      "2.954066276550293\n",
      "2.897813081741333\n",
      "3.0158193111419678\n",
      "3.3149545192718506\n",
      "3.328611135482788\n",
      "3.0547149181365967\n",
      "3.1079018115997314\n",
      "3.2131965160369873\n",
      "3.233023166656494\n",
      "3.5039563179016113\n",
      "3.6217572689056396\n",
      "3.7203283309936523\n",
      "3.617011785507202\n",
      "3.7675893306732178\n",
      "3.2489893436431885\n",
      "3.346986770629883\n",
      "3.491534948348999\n",
      "4.237131595611572\n",
      "3.7500011920928955\n",
      "3.3629872798919678\n",
      "3.241330623626709\n",
      "3.3101251125335693\n",
      "3.1845335960388184\n",
      "3.237740993499756\n",
      "3.148974657058716\n",
      "3.359135150909424\n",
      "3.4952831268310547\n",
      "3.196397304534912\n",
      "3.251753568649292\n",
      "3.6304376125335693\n",
      "3.6222445964813232\n",
      "3.0444021224975586\n",
      "3.3261566162109375\n",
      "3.6058313846588135\n",
      "3.168057918548584\n",
      "3.2240614891052246\n",
      "3.299428701400757\n",
      "3.351862907409668\n",
      "3.282175302505493\n",
      "3.1925296783447266\n",
      "3.6539855003356934\n",
      "3.370931386947632\n",
      "3.1498215198516846\n",
      "3.145188331604004\n",
      "3.2011797428131104\n",
      "3.1743783950805664\n",
      "3.7835206985473633\n",
      "3.8197596073150635\n",
      "3.517271041870117\n",
      "3.6442394256591797\n",
      "3.3741824626922607\n",
      "3.138418674468994\n",
      "3.1052181720733643\n",
      "3.0749213695526123\n",
      "3.157957077026367\n",
      "3.0399789810180664\n",
      "3.3519930839538574\n",
      "3.378743886947632\n",
      "3.448739528656006\n",
      "3.599637746810913\n",
      "3.2793495655059814\n",
      "3.239975929260254\n",
      "3.394899845123291\n",
      "3.618727207183838\n",
      "3.4118545055389404\n",
      "3.2375693321228027\n",
      "3.273599863052368\n",
      "3.15816593170166\n",
      "3.307610511779785\n",
      "3.1911377906799316\n",
      "3.179924964904785\n",
      "3.2054271697998047\n",
      "3.2490320205688477\n",
      "3.4295284748077393\n",
      "3.0947370529174805\n",
      "3.0720856189727783\n",
      "3.082451820373535\n",
      "3.129880428314209\n",
      "3.281869411468506\n",
      "3.343234062194824\n",
      "3.2056713104248047\n",
      "3.187847375869751\n",
      "3.110974073410034\n",
      "3.366950750350952\n",
      "3.518298625946045\n",
      "3.058361291885376\n",
      "3.0670974254608154\n",
      "3.0780370235443115\n",
      "3.1074883937835693\n",
      "3.5429840087890625\n",
      "3.6295125484466553\n",
      "3.696974277496338\n",
      "3.683444023132324\n",
      "3.263253688812256\n",
      "3.082991123199463\n",
      "3.043745517730713\n",
      "3.0590736865997314\n",
      "3.047370195388794\n",
      "3.1666877269744873\n",
      "3.606445074081421\n",
      "3.287886381149292\n",
      "3.140697956085205\n",
      "3.15339994430542\n",
      "3.0718185901641846\n",
      "2.9833922386169434\n",
      "3.361499309539795\n",
      "3.437002182006836\n",
      "3.0864551067352295\n",
      "3.1148531436920166\n",
      "3.5180938243865967\n",
      "3.305149793624878\n",
      "3.0029821395874023\n",
      "3.487351894378662\n",
      "3.7900478839874268\n",
      "3.5389466285705566\n",
      "3.1502513885498047\n",
      "3.2802693843841553\n",
      "3.2058145999908447\n",
      "2.9929165840148926\n",
      "3.0908758640289307\n",
      "3.439415693283081\n",
      "3.3250863552093506\n",
      "3.0987915992736816\n",
      "3.0486562252044678\n",
      "2.980356216430664\n",
      "3.096966028213501\n",
      "3.559598445892334\n",
      "3.679121732711792\n",
      "3.089592218399048\n",
      "3.5450589656829834\n",
      "3.470266342163086\n",
      "3.2452149391174316\n",
      "3.246029853820801\n",
      "3.1552670001983643\n",
      "3.434868097305298\n",
      "3.6785364151000977\n",
      "3.614776372909546\n",
      "3.2460813522338867\n",
      "3.2156383991241455\n",
      "3.0515048503875732\n",
      "3.2080609798431396\n",
      "3.6736180782318115\n",
      "3.669175148010254\n",
      "3.2781293392181396\n",
      "3.103865623474121\n",
      "3.092391014099121\n",
      "3.194753646850586\n",
      "3.1132891178131104\n",
      "3.1532676219940186\n",
      "3.2840230464935303\n",
      "3.255000352859497\n",
      "3.1156833171844482\n",
      "3.199105739593506\n",
      "3.195021867752075\n",
      "3.1765964031219482\n",
      "3.0389626026153564\n",
      "2.986180067062378\n",
      "3.03707218170166\n",
      "3.121340751647949\n",
      "3.643725633621216\n",
      "3.4289257526397705\n",
      "3.565769910812378\n",
      "3.916898250579834\n",
      "3.652738332748413\n",
      "3.7059357166290283\n",
      "3.4734716415405273\n",
      "3.1262617111206055\n",
      "3.0967342853546143\n",
      "3.141035556793213\n",
      "3.4670586585998535\n",
      "3.301082134246826\n",
      "3.276742458343506\n",
      "3.188028335571289\n",
      "3.2229361534118652\n",
      "3.1685879230499268\n",
      "3.2071280479431152\n",
      "3.2629668712615967\n",
      "3.31968355178833\n",
      "3.427617073059082\n",
      "3.1678812503814697\n",
      "3.1096081733703613\n",
      "3.180924654006958\n",
      "3.178386926651001\n",
      "3.4301061630249023\n",
      "3.261406183242798\n",
      "3.1719272136688232\n",
      "3.5094492435455322\n",
      "3.2424330711364746\n",
      "3.1761879920959473\n",
      "3.1561646461486816\n",
      "3.1903328895568848\n",
      "3.2574591636657715\n",
      "3.152820110321045\n",
      "3.0373141765594482\n",
      "2.9715640544891357\n",
      "2.9625535011291504\n",
      "2.9016125202178955\n",
      "2.938610792160034\n",
      "3.031515598297119\n",
      "3.0493552684783936\n",
      "3.031522512435913\n",
      "3.080972671508789\n",
      "3.3794057369232178\n",
      "3.6557791233062744\n",
      "3.104743719100952\n",
      "3.008664608001709\n",
      "2.95943546295166\n",
      "3.067622661590576\n",
      "2.9116339683532715\n",
      "3.1028664112091064\n",
      "3.982297658920288\n",
      "3.420823574066162\n",
      "2.918208360671997\n",
      "2.8616020679473877\n",
      "2.980565071105957\n",
      "3.069740056991577\n",
      "2.9817452430725098\n",
      "3.3398473262786865\n",
      "3.558711290359497\n",
      "3.261273145675659\n",
      "3.3664329051971436\n",
      "3.1340465545654297\n",
      "2.875317335128784\n",
      "2.8955931663513184\n",
      "3.016319513320923\n",
      "3.092651128768921\n",
      "3.0914766788482666\n",
      "2.9675097465515137\n",
      "3.1080973148345947\n",
      "3.186732530593872\n",
      "3.002159357070923\n",
      "3.056361198425293\n",
      "2.9856183528900146\n",
      "2.9187490940093994\n",
      "3.0097439289093018\n",
      "3.2760939598083496\n",
      "3.0239346027374268\n",
      "2.883246660232544\n",
      "2.9119720458984375\n",
      "2.929478645324707\n",
      "3.1426947116851807\n",
      "3.0817370414733887\n",
      "2.9755802154541016\n",
      "3.1302289962768555\n",
      "3.014512300491333\n",
      "3.0400478839874268\n",
      "2.9617700576782227\n",
      "2.9227991104125977\n",
      "3.475745439529419\n",
      "3.250631093978882\n",
      "3.272456169128418\n",
      "3.0362372398376465\n",
      "2.9145712852478027\n",
      "2.9446423053741455\n",
      "3.0298895835876465\n",
      "2.8807010650634766\n",
      "3.0774621963500977\n",
      "3.7156805992126465\n",
      "3.3527638912200928\n",
      "3.3196210861206055\n",
      "3.321075677871704\n",
      "3.015392780303955\n",
      "3.1504764556884766\n",
      "3.434546709060669\n",
      "2.9024007320404053\n",
      "2.923921823501587\n",
      "2.859285831451416\n",
      "2.8824920654296875\n",
      "3.0149805545806885\n",
      "3.1010208129882812\n",
      "3.0828142166137695\n",
      "3.137575626373291\n",
      "3.7270560264587402\n",
      "3.1441640853881836\n",
      "2.7939658164978027\n",
      "3.001929998397827\n",
      "3.220723867416382\n",
      "3.188300371170044\n",
      "3.1801912784576416\n",
      "3.111480712890625\n",
      "3.0147762298583984\n",
      "2.939486026763916\n",
      "2.9605166912078857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-b07782b6ec45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-43721ef0ea6e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, epochs, eval_every, batch_size, seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_trainer.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* Write code to generate next character from this.\n",
    "* Write early stopping code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section name: Grokking Advanced Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same data source as LSTM: \n",
    "\n",
    "* TODO: Draw computational graph\n",
    "* TODO: Describe input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Turing Machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
