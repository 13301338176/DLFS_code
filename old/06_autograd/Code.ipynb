{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_) -> Tuple[Tensor]:\n",
    "        \n",
    "        self.input_ = input_\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, output_grad) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self._input_grads(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self, output_grad) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ParamOperation(Operation):\n",
    "\n",
    "    def __init__(self, param: Tensor) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.output, output_grad)\n",
    "\n",
    "        self.input_grads = self._input_grads(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _param_grad(self, output_grad) -> Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_in: int = 2,\n",
    "                 n_out: int = 1):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "    def forward(self, \n",
    "                inputs: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        \n",
    "        assert len(inputs) == n_in\n",
    "        \n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        assert len(self.outputs) == n_out\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, \n",
    "                 output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self._input_grads(output_grads)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self,\n",
    "                     output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Operation):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_in: int = 1,\n",
    "                 n_out: int = 1):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "    def forward(self, \n",
    "                inputs: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        \n",
    "        assert len(inputs) == n_in\n",
    "        \n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        assert len(self.outputs) == n_out\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, \n",
    "                 output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self._input_grads(output_grads)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self,\n",
    "                     output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1,2,3,4])\n",
    "b = Tensor([2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Add' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-71893817dc5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Add' is not defined"
     ]
    }
   ],
   "source": [
    "op1 = Add(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6977c75dc8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mothers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6977c75dc8f5>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mothers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "others = tuple(x for x in c if x != a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without operations\n",
    "torch.manual_seed(122418)\n",
    "a1 = torch.randn(3,3)\n",
    "w1 = torch.randn(3,3)\n",
    "\n",
    "a2 = torch.randn(3,3)\n",
    "w2 = torch.randn(3,3)\n",
    "\n",
    "b1 = torch.mm(a1, w1)\n",
    "b2 = torch.mm(a2, w2)\n",
    "\n",
    "w3 = torch.randn(3,3)\n",
    "\n",
    "c1 = b1 + b2\n",
    "L = b2 * c1 * w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7283e+00, -3.1867e+01, -1.7938e-03],\n",
       "        [-4.6408e-01, -7.3601e-01, -7.7959e-01],\n",
       "        [ 1.2042e+00,  3.0170e+00, -2.3690e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with operations\n",
    "\n",
    "```python\n",
    "a1 = torch.randn(3,3)\n",
    "w1 = torch.randn(3,3)\n",
    "\n",
    "a2 = torch.randn(3,3)\n",
    "w2 = torch.randn(3,3)\n",
    "\n",
    "w3 = torch.randn(3,3)\n",
    "\n",
    "# operations\n",
    "wm1 = WeightMultiply(w1)\n",
    "wm2 = WeightMultiply(w2)\n",
    "add2 = Add(2, 1)\n",
    "mult3 = Mult(3, 1)\n",
    "\n",
    "b1 = wm1.forward(a1)\n",
    "b2 = wm2.forward(a2)\n",
    "c1 = add2.forward((b1, b2))\n",
    "L = mult3.forward((c1, b2, w3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward\n",
    "\n",
    "```python\n",
    "c1_grad, b2_grad_1, w3_grad = mult3.backward((L_grad))\n",
    " \n",
    "b1_grad, b2_grad_2 = add2.backward(c1_grad)\n",
    "\n",
    "# combine these gradients to reflect the fact that b2 is used twice on the\n",
    "# forward pass\n",
    "b2_grad = b2_grad_1 + b2_grad_2\n",
    "\n",
    "a2_grad = wm2.backward(b2_grad)\n",
    "\n",
    "a1_grad = wm1.backward(b1_grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3\n",
    "a.__add__(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = array(3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition using '__add__': [7 7]\n",
      "Addition using '+': [7 7]\n"
     ]
    }
   ],
   "source": [
    "a = array([3,3])\n",
    "print(\"Addition using '__add__':\", a.__add__(4))\n",
    "print(\"Addition using '+':\", a + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(t: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor):\n",
    "        return t.data.sum()\n",
    "\n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * np.ones_like(t.data)\n",
    "\n",
    "    data = _forward(t)\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Addable = Union[float, int]\n",
    "\n",
    "Numberable = Union[Addable, float, int]\n",
    "\n",
    "def ensure_Number(num: Numberable):\n",
    "    if isinstance(num, NumberWithGrad):\n",
    "        return num\n",
    "    else:\n",
    "        return NumberWithGrad(num)        \n",
    "\n",
    "class NumberWithGrad(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num: Addable,\n",
    "                 depends_on: List[Addable] = None,\n",
    "                 creation_op: str = ''):\n",
    "        self.num = num\n",
    "        self.grad = None\n",
    "        self.depends_on = depends_on or []\n",
    "        self.creation_op = creation_op\n",
    "\n",
    "    def __add__(self, \n",
    "                other: Numberable):\n",
    "        return NumberWithGrad(self.num + ensure_Number(other).num,\n",
    "                              depends_on = [self, ensure_Number(other)],\n",
    "                              creation_op = 'add')\n",
    "    \n",
    "    def __mul__(self,\n",
    "                other: Numberable = None):\n",
    "\n",
    "        return NumberWithGrad(self.num * ensure_Number(other).num,\n",
    "                              depends_on = [self, ensure_Number(other)],\n",
    "                              creation_op = 'mul')\n",
    "    def sum(self):\n",
    "\n",
    "        return special_sum(self)\n",
    "    \n",
    "    def backward(self, backward_grad: Addable = None):\n",
    "        if backward_grad is None: # first time calling backward\n",
    "            self.grad = 1\n",
    "        else: \n",
    "            if self.grad is None:\n",
    "                self.grad = backward_grad\n",
    "            else:\n",
    "                self.grad += backward_grad\n",
    "        \n",
    "        if(self.creation_op == \"add\"):\n",
    "            self.depends_on[0].backward(self.grad)\n",
    "            self.depends_on[1].backward(self.grad)    \n",
    "\n",
    "        if(self.creation_op == \"mul\"):\n",
    "            \n",
    "            new = self.depends_on[1] * self.grad\n",
    "            self.depends_on[0].backward(new.num)\n",
    "            new = self.depends_on[0] * self.grad\n",
    "            self.depends_on[1].backward(new.num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = NumberWithGrad(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a * 4\n",
    "c = b + 3\n",
    "d = (a + 2)\n",
    "e = c * d \n",
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ d = (4a + 3) * (a + 2) = 4a^2 + 11a + 6 $$\n",
    "$$ \\frac{\\partial d}{\\partial a} = 8a + 11 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(num: int):\n",
    "    a = NumberWithGrad(num)\n",
    "    b = a * 4\n",
    "    c = b + 3\n",
    "    return c * (a + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.99999999999943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(forward(3.01) - forward(2.99)) / 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 depends_on: List[Dependency] = None):\n",
    "    \n",
    "        self.data = data\n",
    "        self.depends_on = depends_on or []\n",
    "        self.shape = self.data.shape\n",
    "\n",
    "    def __mul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(self, ensure_tensor(other))\n",
    "\n",
    "    def __rmul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(ensure_tensor(other), self)\n",
    "    \n",
    "    def sum(self) -> 'Tensor':\n",
    "        return tensor_sum(self)\n",
    "    \n",
    "def _mul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    # assertion specific to this function\n",
    "    assert t1.shape == t2.shape\n",
    "\n",
    "    # _forward method, defined for each function like this\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data * t2.data\n",
    "\n",
    "    # gradient method for the first Tensor\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # assertion specific to the \"_mul\" method\n",
    "        assert grad.shape == t2.shape\n",
    "        grad = grad * t2.data\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # assertion specific to the \"_mul\" method\n",
    "        assert grad.shape == t1.shape\n",
    "        grad = grad * t1.data\n",
    "        return grad\n",
    "\n",
    "    # compute the output and define the list of dependencies\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "\n",
    "    # return a Tensor based on these\n",
    "    return Tensor(data, depends_on)\n",
    "   \n",
    "    \n",
    "def _matmul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    assert t1.shape[1] == t2.shape[0]\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data @ t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = grad @ t2.data.T\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = t1.data.T @ grad\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lincoln.autograd.tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[1,1], [1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b * b * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[18. 18.]\n",
       " [18. 18.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y * y * 3\n",
    "out = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
