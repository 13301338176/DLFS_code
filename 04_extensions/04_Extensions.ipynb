{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "1. Softmax cross entropy loss\n",
    "    * Softmax\n",
    "    * Log loss\n",
    "1. Something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=2)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Normalize}(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix})  = \\begin{bmatrix} \\frac{x_1}{x_1 + x_2 + x_3} \\\\ \n",
    "\\frac{x_2}{x_1 + x_2 + x_3} \\\\\n",
    "\\frac{x_3}{x_1 + x_2 + x_3}\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Softmax}(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix})  = \\begin{bmatrix} \\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}} \\\\ \n",
    "\\frac{e^{x_2}}{e^{x_1} + e^{x_2} + e^{x_3}} \\\\\n",
    "\\frac{e^{x_3}}{e^{x_1} + e^{x_2} + e^{x_3}}\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall log loss formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i}{(- y_i * log(p_i) - (1 - y_i) * log(1-p_i))} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: $y_i = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula becomes:\n",
    "\n",
    "$$ \\begin{align} &\\sum_{i}{(- 0 * log(p_i) - (1 - 0) * log(1-p_i))} \\\\\n",
    "=  &\\sum_{i}{- log(1-p_i))} \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of loss when $y_i = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 0.99, 99)\n",
    "y1 = -1.0 * np.log(1 - x)\n",
    "y2 = (x - 0) ** 2\n",
    "plt.plot(x, y1);\n",
    "plt.plot(x, y2);\n",
    "plt.legend(['Cross entropy loss', 'Mean squared error'])\n",
    "\n",
    "plt.title(\"Cross entropy loss vs. MSE when $y = 0$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Loss values\");\n",
    "plt.savefig(\"Log_loss_vs_MSE_y_eq_0.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $y=0$, loss can become theoretically infinite as $p$ approaches 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: $y_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula reduces to:\n",
    "\n",
    "$$ \\begin{align} &\\sum_{i}{(- 1 * log(p_i) - (1 - 1) * log(1-p_i))} \\\\\n",
    "=  &\\sum_{i}{- log(p_i)} \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of loss when $y_i = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 0.99, 99)\n",
    "y = -1.0 * np.log((x))\n",
    "\n",
    "plt.plot(x, y);\n",
    "plt.title(\"Log loss for $y = 1$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Log loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of softmax loss when $y_i = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 0.99, 99)\n",
    "y = -1.0 * np.log(1 - (np.exp(x) / (np.exp(x) + np.exp(0.5))))\n",
    "plt.plot(x, y);\n",
    "\n",
    "plt.title(\"Log loss when $y = 0$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Log loss\")\n",
    "plt.savefig(\"log_loss_y_0.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of softmax loss when $y_i = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 0.99, 99)\n",
    "y = -1.0 * np.log((np.exp(x) / (np.exp(x) + np.exp(0.5))))\n",
    "\n",
    "plt.plot(x, y);\n",
    "plt.title(\"Log loss for $y = 1$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Log loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $y=0$, loss can become theoretically infinite as $p$ approaches 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of softmax loss cross derivative when $y_i = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 0.99, 99)\n",
    "y = -1.0 * np.log((np.exp(0.5) / (np.exp(x) + np.exp(0.5))))\n",
    "\n",
    "plt.plot(x, y);\n",
    "plt.title(\"Log loss for $y = 1$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Log loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition: since the sum of all $y$s is 1, increasing x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from below when y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(4, 5, 100)\n",
    "y = -1.0 * np.log(1 - (np.exp(5.2) / (np.exp(x) + np.exp(5.2) + np.exp(3))))\n",
    "plt.plot(x, y);\n",
    "\n",
    "plt.title(\"Log loss when $y = 0$\")\n",
    "plt.xlabel(\"Prediction ($p$)\")\n",
    "plt.ylabel(\"Log loss\")\n",
    "plt.savefig(\"log_loss_y_0.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition: the actual prediction from this probability, $p_2$ was 4.1, which translated into a softmax probability of about 0.29. The actual value was 0, so you would think that increasing this probability from 4.1 to some other value would increase the loss. \n",
    "\n",
    "However, the real problem with this loss vector is that there is another probability, $p_3$, of 5.2, which translates into an even higher softmax probability and also has a target value of 0. And increasing $p_2$ actually decreases the ultimate probability value for $p_3$. So overall, to increase the value of the cross entropy loss, increasing $p_2$ is called for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically stable softmax math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log(softmax(x_j)) = log(\\frac{e^{x_j}}{\\sum_i^n e^{x_j}}) = x_j - logsumexp(X) $$\n",
    "\n",
    "$$ e^{x - logsumexp(x)} = softmax(x) $$\n",
    "\n",
    "$$ logsumexp(X) = log(\\sum_i^n e^{x_i}) = c + log(\\sum_i^n e^{x_i - c}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lincoln.np_utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([5,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a / np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,1.5],[3,3.5]])\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "softmax(a, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cross entropy derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here: https://www.wolframalpha.com/input/?i=derivative+of+-a+*+log((e%5Ex)+%2F+(e%5Ex+%2B+b))+-+(1+-+a)+*+log(1+-+(e%5Ex)+%2F+(e%5Ex+%2B+b))\n",
    "\n",
    "and here: https://www.wolframalpha.com/input/?i=derivative+of+(-a+*+log((b+%2F+(e%5Ex+%2B+b)))+-+(1+-+a)+*+log(1+-+b+%2F+(e%5Ex+%2B+b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {SCE}_1 = -y_1 * log(\\frac{e^{p_1}}{e^{p_1} + e^{p_2} + e^{p_3}}) - (1-y_1) * log(1 - \\frac{e^{p_1}}{e^{p_1} + e^{p_2} + e^{p_3}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Wolfram Alpha:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_1}{\\partial p_2} = -\\frac{(y_1 - 1) * e^{p_3}}{e^{p_1} + e^{p_3}} + y_1 + \\frac{-e^{p_2} - e^{p_3}}{e^{p_1} + e^{p_2} + e^{p_3}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_2}{\\partial p_1} = -\\frac{(y_2 - 1) * e^{p_3}}{e^{p_2} + e^{p_3}} + y_2 + \\frac{-e^{p_1} - e^{p_3}}{e^{p_1} + e^{p_2} + e^{p_3}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 2.7\n",
    "p2 = 2.9\n",
    "p3 = 3.4\n",
    "\n",
    "y1 = 0\n",
    "y2 = 0\n",
    "y3 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([[p1, p2, p3], [3, 4.1, 5.2]])\n",
    "y = np.array([[y1, y2, y3], [1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = softmax(p, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = p[0]\n",
    "y0 = y[0]\n",
    "softmax(p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_test(ind):\n",
    "    sp0 = softmax(p0)\n",
    "    ce = -y0 * np.log(sp0) - (1 - y0) * (np.log(1 - sp0))\n",
    "\n",
    "    p1 = p0.copy()\n",
    "    p1[ind] += 0.01\n",
    "    sp1 = softmax(p1)\n",
    "    ce1 = -y0 * np.log(sp1) - (1 - y0) * (np.log(1 - sp1))\n",
    "\n",
    "    return (ce1 - ce) / 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff_test(0) \n",
    "# partial SCE2 / partial p1 = -0.1\n",
    "# partial SCE3 / partial p1 = 0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {SCE}_1 = -y_1 * log(\\frac{e^{p_1}}{e^{p_1} + e^{p_2} + e^{p_3}}) - (1-y_1) * log(1 - \\frac{e^{p_1}}{e^{p_1} + e^{p_2} + e^{p_3}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_1}{\\partial p_1} = \\frac{e^{p_1}}{e^{p_1} + e^{p_2} + e^{p_3}} - y_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_2}{\\partial p_1} = -\\frac{(y_2 - 1) * e^{p_3}}{e^{p_1} + e^{p_3}} + y_2 + \\frac{-e^{p_2} - e^{p_3}}{e^{p_1} + e^{p_2} + e^{p_3}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-((y2 - 1) * np.exp(p3)) / (np.exp(p1) + np.exp(p3)) + y2 + (-np.exp(p2) - np.exp(p3)) / (np.exp(p1) + np.exp(p2) + np.exp(p3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_3}{\\partial p_1} = -\\frac{(y_3 - 1) * e^{p_2}}{e^{p_2} + e^{p_3}} + y_3 + \\frac{-e^{p_1} - e^{p_2}}{e^{p_1} + e^{p_2} + e^{p_3}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-((y3 - 1) * np.exp(p2)) / (np.exp(p1) + np.exp(p2)) + y3 + (-np.exp(p2) - np.exp(p3)) / (np.exp(p1) + np.exp(p2) + np.exp(p3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_i}{\\partial p_j} = (1 - \\frac{e^{p_i}}{\\sum_{k \\neq j} e^{p_k}}) * (y_i - 1) + y_i +(\\frac{e^{p_j}}{\\sum_{k} e^{p_k}} - 1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_ratios(ps):\n",
    "    '''\n",
    "    Calc (1 - (e^ind2))/(sum(e^i) i != ind2) \n",
    "    '''\n",
    "    out_list = []\n",
    "\n",
    "    b = np.zeros_like(ps, dtype=float)\n",
    "    for i in range(len(ps)):\n",
    "        temp = np.delete(ps, i) # p1, p3\n",
    "        s = np.array([np.exp(t) for t in temp]).sum() # sum of exps\n",
    "        b[i] = s # set to b\n",
    "        # b[0] = e^p1 + e^p2\n",
    "        # b[1] = e^p0 + e^p2\n",
    "\n",
    "    c = np.zeros((ps.shape[0], ps.shape[0])) # for p1, all the other values\n",
    "    for i in range(len(ps)): # SCE subscript\n",
    "        for j in range(len(ps)): # p subscript\n",
    "            c[i][j] = 1 - (np.exp(ps[i]) / b[j]) # e.g. for 2, 1 - (p1 / (e^p1 + e^p3))\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{bmatrix} \\frac{\\partial {SCE}_1}{\\partial p_1} & \\frac{\\partial {SCE}_2}{\\partial p_1} & \\frac{\\partial {SCE}_3}{\\partial p_1} \\\\ \\frac{\\partial {SCE}_1}{\\partial p_2} & \\frac{\\partial {SCE}_2}{\\partial p_2} & \\frac{\\partial {SCE}_3}{\\partial p_2} \\\\\n",
    "\\frac{\\partial {SCE}_1}{\\partial p_3} & \\frac{\\partial {SCE}_2}{\\partial p_3} & \\frac{\\partial {SCE}_3}{\\partial p_3}\n",
    "\\end{bmatrix}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_2}{\\partial p_1} = -\\frac{(y_2 - 1) * e^{p_3}}{e^{p_1} + e^{p_3}} + y_2 + \\frac{-e^{p_2} - e^{p_3}}{e^{p_1} + e^{p_2} + e^{p_3}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{exp_ratios}[0][1] + sp[0] + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying calculation produces the same value when using `exp_ratios`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ratio = exp_ratios(p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial {SCE}_1}{\\partial p_2} = -\\frac{(y_1- 1) * e^{p_3}}{e^{p_2} + e^{p_3}} + y_1 + \\frac{-e^{p_1} - e^{p_3}}{e^{p_1} + e^{p_2} + e^{p_3}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_ratio[0][1] + sp[0][0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_grads_calc(ps, ys):\n",
    "    prob_grads = []\n",
    "    softmax_preds = softmax(ps, axis=1)\n",
    "    batch_size = softmax_preds.shape[0]\n",
    "    num_features = softmax_preds.shape[1]\n",
    "    for n in range(batch_size):\n",
    "        exp_ratio = exp_ratios(p[n])\n",
    "        jacobian = np.zeros((num_features, num_features))\n",
    "        for f1 in range(num_features): # p index\n",
    "            for f2 in range(num_features): # SCE index\n",
    "                if f1 == f2:\n",
    "                    jacobian[f1][f2] = (\n",
    "                        softmax_preds[n][f1] - ys[n][f1])\n",
    "                else:\n",
    "                    jacobian[f1][f2] = (\n",
    "                        -(ys[n][f2]-1) * exp_ratio[f1][f2] + ys[n][f2] + softmax_preds[n][f1] - 1)\n",
    "        print(jacobian)\n",
    "        prob_grads.append(jacobian.sum(axis=1))\n",
    "\n",
    "    return np.stack(prob_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 2.7\n",
    "p2 = 2.9\n",
    "p3 = 3.4\n",
    "\n",
    "y1 = 0\n",
    "y2 = 0\n",
    "y3 = 1\n",
    "\n",
    "p = np.array([[p1, p2, p3], [3, 4.1, 5.2]])\n",
    "y = np.array([[y1, y2, y3], [1, 0, 0]])\n",
    "print(softmax(p, axis=1))\n",
    "prob_grads_calc(p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_indices(p, y, ind1, ind2):\n",
    "    sp = softmax(p, axis=1)\n",
    "    ce = -y * np.log(sp) - (1 - y) * (np.log(1 - sp))\n",
    "    cesum = ce.sum()\n",
    "    \n",
    "    p1 = p.copy()\n",
    "    p1[ind1][ind2] += 0.01\n",
    "    sp1 = softmax(p1, axis=1)\n",
    "    ce1 = -y * np.log(sp1) - (1 - y) * (np.log(1 - sp1))\n",
    "    ce1sum = ce1.sum()\n",
    "    \n",
    "    return (ce1sum - cesum) / 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{bmatrix} p_1 \\\\ p_2 \\\\ p_3 \\\\ \\vdots \\\\ p_n \\end{bmatrix} \\Rightarrow \\begin{bmatrix} p_1 & 1-p_1 \\\\ p_2 & 1-p_2 \\\\ p_3 & 1-p_3 \\\\ \\vdots & \\vdots \\\\ p_n & 1-p_n \\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Opposite:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{bmatrix} p_1 & 1-p_1 \\\\ p_2 & 1-p_2 \\\\ p_3 & 1-p_3 \\\\ \\vdots & \\vdots \\\\ p_n & 1-p_n \\end{bmatrix}_{grad} \\Rightarrow \\begin{bmatrix} p_1 \\\\ p_2 \\\\ p_3 \\\\ \\vdots \\\\ p_n \\end{bmatrix}_{grad} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_indices(p, y, 1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "$$ [0, 2, 1] \\Rightarrow \\begin{bmatrix} 1 & 0 & 0 & \\ldots & 0 \\\\ 0 & 0 & 1 & \\ldots & 0 \\\\ 0 & 1 & 0 & \\ldots & 0 \\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\alpha_t = \\alpha * \\delta^t $$\n",
    "\n",
    "$$ 0 \\leq \\delta \\leq 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\alpha_t = \\alpha_{start} - (\\alpha_{start} - \\alpha_{end}) * \\frac{t}{N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition (Redo!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If y = 1, $ \\frac{\\partial {SCE}_1}{\\partial p_2} $ is softmax(1).\n",
    "\n",
    "If y = 0, $ \\frac{\\partial {SCE}_1}{\\partial p_2} $ is: \n",
    "\n",
    "$$ \\frac{e^{p_3}}{e^{p_1} + e^{p_3}} + (softmax(1) - 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the proportion of $p_1$ is of the overall proportion, the less the loss increases.\n",
    "\n",
    "If $p_3$ is massive relative to $p_1$, and $y=0$, then increasing $p_1$ will increase the loss more.\n",
    "\n",
    "Reasoning: loss will reduce to: \n",
    "$$ {SCE}_2 = -log(1 - \\frac{e^{p_2}}{e^{p_1} + e^{p_2} + e^{p_3}}) $$\n",
    "\n",
    "and smaller $p_2$ means this value is closer to 1, which means increasing $p_1$ will take it away from 1 fast. \n",
    "\n",
    "since we'll be increasing the fraction $\\frac{e^{p_1}}{e^{p_1} + e^{p_2} + e^{p_3}}$ from a lower base.\n",
    "\n",
    "The higher $softmax(1)$, the more increasing $p_1$ increases\n",
    "\n",
    "This is again because of the nature of the exponential function: the higher it starts, the more impact increasing it has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Momentum gradient formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{update} = \\nabla_t + \\mu * \\nabla_{t-1} + \\mu^2 * \\nabla_{t-2} + \\ldots $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9 ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight init illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 784\n",
    "n_hidden = 256\n",
    "\n",
    "np.random.seed(190131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(1, n_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.randn(n_feat, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  6., 20., 43., 42., 61., 40., 23., 11.,  5.]),\n",
       " array([-75.67, -60.77, -45.86, -30.96, -16.06,  -1.16,  13.75,  28.65,\n",
       "         43.55,  58.46,  73.36]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.dot(a, b).reshape(n_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch norm math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incrementally update variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(existingAggregate: Tuple[np.ndarray], \n",
    "           newValue: np.ndarray):\n",
    "\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    count += 1 \n",
    "    delta = newValue - mean\n",
    "    mean += delta / count\n",
    "    delta2 = newValue - mean\n",
    "    M2 += delta * delta2\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "def finalize(existingAggregate: Tuple[np.ndarray]):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    (mean, variance, sampleVariance) = (mean, M2/count, M2/(count - 1)) \n",
    "    if count < 2:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return (mean, variance, sampleVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With 2 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20190110)\n",
    "a = np.random.randn(1,3)\n",
    "b = np.random.randn(1,3)\n",
    "print(a)\n",
    "print(b)\n",
    "list1 = [a, b]\n",
    "stats = (0, np.zeros_like(a), np.zeros_like(a))\n",
    "for el in list1:\n",
    "    stats = update(stats, el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalize(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(np.array(list1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With 1 function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20190110)\n",
    "a = np.random.randn(1,3)\n",
    "b = np.random.randn(1,3)\n",
    "print(a)\n",
    "print(b)\n",
    "list1 = [a, b]\n",
    "stats = (0, np.zeros_like(a), np.zeros_like(a))\n",
    "for el in list1:\n",
    "    stats = update(stats, el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int) -> None:\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[Tensor] = []\n",
    "        self.param_grads: List[Tensor] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, input_: Tensor) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        assert_same_shapes(self.output, output_grad)\n",
    "\n",
    "        for operation in self.operations[::-1]:\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "\n",
    "        assert_same_shapes(self.input_, input_grad)\n",
    "\n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> None:\n",
    "\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> None:\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchNorm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "a / a.var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(32, 3, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a - a.mean(axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "        \n",
    "    def _setup_layer(self, input_: np.ndarray) -> None:\n",
    "        obs = input_[0]\n",
    "        \n",
    "        self.aggregates = (0,\n",
    "                           np.zeros_like(obs),\n",
    "                           np.zeros_like(obs))\n",
    "\n",
    "        self.params: List[float] = []\n",
    "        self.params.append(0.)\n",
    "        self.params.append(1.)\n",
    "    \n",
    "    def _update_stats(self, new_input: np.ndarray):\n",
    "\n",
    "        (count, mean, M2) = self.aggregates\n",
    "        count += 1 \n",
    "        delta = new_input - mean\n",
    "        mean += delta / count\n",
    "        delta2 = new_input - mean\n",
    "        M2 += delta * delta2\n",
    "        \n",
    "        self.aggregates = (count, mean, M2)\n",
    "            \n",
    "        \n",
    "    def forward(self, input_: np.ndarray,\n",
    "                inference=False) -> np.ndarray:\n",
    "\n",
    "        self.input_ = input_\n",
    "        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n",
    "        if not inference:\n",
    "            for obs in input_:\n",
    "                self._update_stats(obs)\n",
    "\n",
    "            self.mean = input_.mean(axis=0)\n",
    "            self.var = input_.var(axis=0)\n",
    "        else:\n",
    "            self.mean, self.var, samp_var = finalize(self.aggregates)             \n",
    "\n",
    "        self.output = (input_ - self.mean) / (self.var + 1e-8)\n",
    "        \n",
    "        self.output *= self.params[0] # gamma\n",
    "        self.output += self.params[0] # beta\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, \n",
    "                 output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        # https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "        dbeta = np.sum(output_grad, axis=0)\n",
    "        dgamma = np.sum((self.input_ - mu) * \\\n",
    "                        np.sqrt((self.var + 1e-8)) * output_grad, axis=0)\n",
    "        \n",
    "        self.param_grads = [dbeta, dgamma]\n",
    "        \n",
    "        input_grad = (self.params[1] * np.sqrt(self.var + 1e-8) / N) * \\\n",
    "                     (N * output_grad - np.sum(output_grad, axis=0) - \\\n",
    "                      (self.input_ - self.mean) * (self.var + 1e-8)**(-1.0) * \\\n",
    "                      np.sum(output_grad * (input_ - self.mean), axis=0))\n",
    "\n",
    "        assert_same_shape(self.input_, input_grad)\n",
    "        \n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably not going to include Batch Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "\n",
    "    def __init__(self, param: Tensor) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        \n",
    "    def _param_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "\n",
    "    def __init__(self, \n",
    "                 W: Tensor, \n",
    "                 param_name: str='W'):\n",
    "        self.param = W\n",
    "        self.param_name = param_name\n",
    "    \n",
    "    def forward(self, \n",
    "                input_: Tensor):\n",
    "        self.input_ = input_\n",
    "\n",
    "        # Lines specific to this layer\n",
    "        assert self.input_.shape[1] == self.param.shape[0], \\\n",
    "        \"Mismatch of shapes in WeightMultiply operation\"\n",
    "    \n",
    "        self.output = torch.mm(input_, self.param)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, \n",
    "                 output_grad: Tensor):\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        # Lines specific to this layer        \n",
    "        input_grad = torch.mm(output_grad, self.param.transpose(0, 1))\n",
    "        \n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, input_grad)\n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grad(self, \n",
    "                    output_grad: Tensor):\n",
    "\n",
    "        # Lines specific to this layer \n",
    "        param_grad = torch.mm(self.input_.transpose(0, 1), output_grad)\n",
    "        \n",
    "        assert_same_shape(self.param, param_grad)\n",
    "        return param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "\n",
    "    def __init__(self, \n",
    "                 B: Tensor,\n",
    "                 param_name: str='B'):\n",
    "        self.param = B\n",
    "        self.param_name = param_name\n",
    "    \n",
    "    def forward(self, \n",
    "                input_: Tensor):\n",
    "        self.input_ = input_\n",
    "        \n",
    "        # Lines specific to this layer         \n",
    "        assert self.input_.shape[1] == self.param.shape[1], \\\n",
    "        \"Mismatch of shapes in BiasAdd operation\"\n",
    "        self.output = torch.add(self.input_, self.param)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, \n",
    "                 output_grad: Tensor):\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        input_grad = torch.ones_like(self.input_) * output_grad\n",
    "        \n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        \n",
    "        assert_same_shape(self.input_, input_grad)\n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grad(self, \n",
    "                   output_grad: Tensor):\n",
    "\n",
    "        param_grad = torch.ones_like(self.param) * output_grad\n",
    "        \n",
    "        param_grad = torch.sum(param_grad, dim=0).reshape(1, param_grad.shape[1])\n",
    "        \n",
    "        assert_same_shape(self.param, param_grad)\n",
    "        return param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "\n",
    "* Optimizers\n",
    "    * Blocker: change all operation, layers, network code to Numpy. \n",
    "    * Test.\n",
    "    * Write optimizers.\n",
    "* Dropout\n",
    "    * Write operation to show an example of using the \"inference\" flag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
