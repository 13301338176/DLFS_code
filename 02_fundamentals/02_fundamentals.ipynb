{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from numpy import ndarray as array\n",
    "from helper import to_2d, tensor_size\n",
    "\n",
    "from typing import Callable, Dict, Tuple, List, NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciKit Learn Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406077428649427"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(data, target)\n",
    "lr.score(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92,  1.08,  0.14,  0.68, -2.06,  2.67,  0.02, -3.1 ,  2.66,\n",
       "       -2.08, -2.06,  0.86, -3.75])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.round(lr.coef_, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = Tensor(data), Tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradients(forward_info) -> Tensor:\n",
    "    \n",
    "    dLdP = -(forward_info['y'] - forward_info['P'])\n",
    "    \n",
    "    dPdN = torch.ones_like(forward_info['N'])\n",
    "    \n",
    "    dLdN = dLdP * dPdN\n",
    "    \n",
    "    dNdW = forward_info['X'].transpose(0, 1)\n",
    "\n",
    "    dLdW = torch.mm(dNdW, dLdN)\n",
    "    \n",
    "    dPdB = torch.ones_like(forward_info['B'])\n",
    "    \n",
    "    dLdB = dLdP * dPdB\n",
    "    \n",
    "    return dLdW, dLdB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "Batch = Tuple[Tensor, Tensor]\n",
    "\n",
    "def generate_batch(X: Tensor, \n",
    "                   y: Tensor,\n",
    "                   start: int = 0,\n",
    "                   batch_size: int = 10) -> Iterator[Batch]:\n",
    "    \n",
    "    assert (X.dim() == 2) and (y.dim() == 2), \\\n",
    "    \"X and Y must be 2 dimensional\"\n",
    "\n",
    "    if start+batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0] - start\n",
    "    \n",
    "    X_batch, y_batch = X[start:start+batch_size], y[start:start+batch_size]\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(X: Tensor,\n",
    "                 y: Tensor,\n",
    "                 W: Tensor,\n",
    "                 B: Tensor) -> Tuple[Dict[str, Tensor], float]:\n",
    "\n",
    "    # For the matrix multiplication to work,\n",
    "    assert X.shape[1] == W.shape[0], \\\n",
    "    \"Dimensions of betas and feature size do not match\"\n",
    "\n",
    "    N = torch.mm(X, W)\n",
    "\n",
    "    P = torch.add(N, B.item())\n",
    "\n",
    "    loss = torch.sum(torch.pow(y - P, 2)).item()\n",
    "\n",
    "    forward_info: Dict[str, Tensor] = {}\n",
    "    forward_info['X'] = X\n",
    "    forward_info['W'] = W\n",
    "    forward_info['B'] = B\n",
    "    forward_info['N'] = N\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "\n",
    "    return forward_info, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X: Tensor, \n",
    "          y: Tensor, \n",
    "          n_iter: int = 1000,\n",
    "          learning_rate: float = 0.01,\n",
    "          batch_size: int = 100,\n",
    "          return_losses: bool = False, \n",
    "          return_weights: bool = False) -> None:\n",
    "\n",
    "    y = to_2d(y, \"col\")\n",
    "    start = 0\n",
    "\n",
    "    # Initialize weights\n",
    "    W = torch.empty(X.shape[1], 1).uniform_(-1, 1)\n",
    "    B = torch.empty(1, 1).uniform_(-1, 1)\n",
    "\n",
    "    if return_losses:\n",
    "        losses = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        if start >= X.shape[0]:\n",
    "            X, y = permute_data(X, y)\n",
    "            start = 0\n",
    "        \n",
    "        X_batch, y_batch = generate_batch(X, y, start, batch_size)\n",
    "\n",
    "        start += batch_size\n",
    "    \n",
    "        forward_info, loss = forward_loss(X_batch, y_batch, W, B)\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "\n",
    "        dLdW, dLdB = loss_gradients(forward_info)\n",
    "        W -= learning_rate * dLdW\n",
    "        B -= learning_rate * torch.sum(dLdB)\n",
    "    \n",
    "    if return_weights:\n",
    "        weights: Dict[str, Tensor] = {}\n",
    "        weights['W'] = W\n",
    "        weights['B'] = B\n",
    "        return losses, weights\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(80718)\n",
    "train_info = train(data, target, \n",
    "                   learning_rate = 0.001,\n",
    "                   batch_size=23, \n",
    "                   return_losses=True, \n",
    "                   return_weights=True)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: Tensor, \n",
    "            y: Tensor, \n",
    "            weights: Dict[str, Tensor]):\n",
    "    \n",
    "    N = torch.mm(X, weights['W'])\n",
    "\n",
    "    return torch.add(N, weights['B'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(data, target, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6174701879681399"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(preds, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_2(learning_rate: float = 0.01, \n",
    "             n_iter: int = 1000) -> float:\n",
    "    torch.manual_seed(80718)\n",
    "    train_info = train(data, target, \n",
    "                       learning_rate=learning_rate,\n",
    "                       batch_size=23, \n",
    "                       n_iter=n_iter,\n",
    "                       return_losses=True, \n",
    "                       return_weights=True)\n",
    "\n",
    "    losses = train_info[0]\n",
    "    weights = train_info[1]\n",
    "\n",
    "    preds = predict(data, target, weights)\n",
    "    \n",
    "    return r2_score(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.geomspace(0.01, 0.00001, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = [r2_score_2(float(lr), 10000) for lr in lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-efe55c224adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.xlim(lrs[-1], lrs[0])\n",
    "plt.semilogx(lrs, r2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(80718)\n",
    "train_info = train(data, target, \n",
    "                   learning_rate=0.0001,\n",
    "                   batch_size=23, \n",
    "                   n_iter=10000,\n",
    "                   return_losses=True, \n",
    "                   return_weights=True)\n",
    "\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]\n",
    "\n",
    "preds = predict(data, target, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogy(list(range(10000)), losses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in range(a.shape[0]): # for each observation in the batch\n",
    "    exp_obs = torch.exp(a[obs])\n",
    "    sum_exp_obs = exp_obs.sum().item()\n",
    "    softmax_obs = exp_obs / sum_exp_obs\n",
    "    print(softmax_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: Tensor, \n",
    "            deriv: bool=False) -> Tensor:\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + torch.exp(-1.0 * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: Tensor) -> Tensor:\n",
    "\n",
    "    assert x.dim() == 2, \\\n",
    "    \"Expect Tensor with shape (batch_size, num_classes), instead \" + \\\n",
    "    \"x has shape {0}\".format(x.shape)\n",
    "    \n",
    "    def _softmax_row(row: Tensor) -> Tensor:\n",
    "        \n",
    "        assert row.dim() == 1, \\\n",
    "        \"'row' should indeed be a row, instead it has shape\" \\\n",
    "        .format(row.shape)\n",
    "        \n",
    "        exp_obs = torch.exp(row)\n",
    "        sum_exp_obs = exp_obs.sum().item()\n",
    "        softmax_obs = exp_obs / sum_exp_obs\n",
    "        \n",
    "        return softmax_obs\n",
    "\n",
    "    output_rows = []\n",
    "    for obs in range(x.shape[0]):\n",
    "        output_row = to_2d(_softmax_row(x[obs]), \"row\")\n",
    "        output_rows.append(output_row)\n",
    "        \n",
    "    return torch.cat(output_rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(Tensor([[10, 8, 6, 4, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "data = breast_cancer.data\n",
    "target = breast_cancer.target\n",
    "features = breast_cancer.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression(C=10e9)\n",
    "logr.fit(data, target)\n",
    "preds = logr.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965\n"
     ]
    }
   ],
   "source": [
    "print(\"{:.3f}\".format(accuracy_score(preds, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972\n"
     ]
    }
   ],
   "source": [
    "print(\"{:.3f}\".format(f1_score(preds, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand rolled logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_2d(predictions: Tensor) -> Tensor:\n",
    "    \n",
    "    assert predictions.shape[1] == 1, \\\n",
    "    \"Expected a column for predictions, got shape: {}\".format(predictions.shape)\n",
    "    \n",
    "    inverse_predictions = 1 - predictions\n",
    "    \n",
    "    return torch.cat([predictions, inverse_predictions], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor, target_tensor = Tensor(data), Tensor(to_2d(Tensor(target), \"col\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor = predictions_to_2d(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standard = standardize_data(data)\n",
    "data_standard = Tensor(data_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_logistic(observations: Tensor,\n",
    "                     betas: Tensor) -> Tensor:\n",
    "    \n",
    "    # For the matrix multiplication to work, \n",
    "    assert observations.shape[1] == betas.shape[0], \\\n",
    "    \"Dimensions of betas and feature size do not match\"\n",
    "    \n",
    "    mult = torch.mm(observations, betas)\n",
    "    predictions = sigmoid(mult)\n",
    "    predictions = predictions_to_2d(predictions)\n",
    "    softmax_preds = softmax(predictions)\n",
    "    \n",
    "    forward_info: Dict[str, Tensor] = {}\n",
    "    forward_info['betas'] = betas\n",
    "    forward_info['observations'] = observations\n",
    "    forward_info['mult'] = mult\n",
    "    forward_info['predictions'] = predictions   \n",
    "    forward_info['softmax'] = softmax_preds   \n",
    "\n",
    "    return forward_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions: Tensor, \n",
    "                  actual: Tensor) -> Tensor:\n",
    "    \n",
    "    assert predictions.shape == actual.shape, \\\n",
    "    \"Prediction and actual must have same shape\"\n",
    "    \n",
    "    return -1.0 * actual * torch.log(predictions) - (1.0 - actual) * torch.log(1 - predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(forward_info: Dict[str, Tensor], \n",
    "         actual: Tensor, \n",
    "         kind: str = \"binary_crossentropy\") -> Tensor:\n",
    "     \n",
    "    assert kind in (\"binary_crossentropy\"), \\\n",
    "    \"Inappropriate loss type\"\n",
    "\n",
    "    predictions = forward_info['softmax']\n",
    "\n",
    "    n = predictions.shape[0]\n",
    "    \n",
    "    ce = cross_entropy(predictions, actual)\n",
    "\n",
    "    return ce.sum().item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bce_softmax_deriv(predictions: Tensor, \n",
    "                           actual: Tensor, \n",
    "                           kind: str = \"binary_crossentropy\", \n",
    "                           softmax: bool=True) -> Tensor:\n",
    "     \n",
    "    assert kind in (\"binary_crossentropy\"), \\\n",
    "    \"Inappropriate loss type\"\n",
    "\n",
    "    assert predictions.shape == actual.shape, \\\n",
    "    \"Prediction and actual must have same shape\"\n",
    "\n",
    "    return to_2d((predictions - actual)[:, 0], \"col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_update_logistic(forward_info: Dict[str, Tensor], \n",
    "                         actual: Tensor, \n",
    "                         softmax: bool=True) -> Tensor:\n",
    "\n",
    "    obs = forward_info['observations'] \n",
    "    predictions = forward_info['softmax']        \n",
    "\n",
    "    n = obs.shape[0]\n",
    "\n",
    "    dLdP = loss_bce_softmax_deriv(predictions, actual)\n",
    "    dPdB = sigmoid(forward_info['mult'], deriv=True)\n",
    "    dLdB = dPdB * dLdP\n",
    "    \n",
    "    dBdA = obs.transpose(0, 1)\n",
    "\n",
    "    return dBdA.mm(dLdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_logistic(data: Tensor, \n",
    "                         target: Tensor, \n",
    "                         learning_rate: float = 0.01, \n",
    "                         num_iter: int = int(1e3)) -> Tensor:\n",
    "    \n",
    "    # Generate random betas\n",
    "    torch.manual_seed(61818)\n",
    "    betas = torch.randn((data.shape[1], 1))\n",
    "\n",
    "    # For each iteration\n",
    "    for i in range(int(num_iter)):\n",
    "        # Generate a random batch\n",
    "        np.random.seed(61818)\n",
    "        x_batch, y_batch = generate_batch(data, target, 100)\n",
    "\n",
    "        # Forward through the network\n",
    "        forward_info = forward_logistic(x_batch, betas)\n",
    "        L = loss(forward_info, y_batch)\n",
    "        if i % int(1e2) == 0:\n",
    "            print(\"Loss after {:d} iterations: {:.3f}\".\n",
    "                  format(i, L))\n",
    "\n",
    "        # Compute the beta update\n",
    "        update = beta_update_logistic(forward_info, y_batch)\n",
    "        \n",
    "        # Update the betas\n",
    "        betas = betas - learning_rate * update \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_pred = train_model_logistic(data_standard, target_tensor, \n",
    "                                 num_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_info = forward_logistic(data_standard, beta_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred_info['predictions'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(predictions[:, 0], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_score(predictions[:, 0], target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this exercise was just to sanity check that our math was correct; that these operations, which we argued through a bunch of equations would lead to us solving a logistic regression problem, actually does lead to that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
