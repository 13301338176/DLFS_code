{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from numpy import ndarray as array\n",
    "from helper import to_2d, tensor_size\n",
    "\n",
    "from typing import Callable, Dict, Tuple, List, NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ALL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciKit Learn Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7197224944944625"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.01,  0.71,  0.28,  0.71, -2.21,  2.37,  0.72, -2.66,  2.63,\n",
       "       -1.82, -2.33,  0.86, -4.2 ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.round(lr.coef_, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(X, y):\n",
    "    perm = torch.randperm(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradients(forward_info: Dict[str, Tensor], \n",
    "                   weights: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
    "    \n",
    "    dLdP = -(forward_info['y'] - forward_info['P'])\n",
    "    \n",
    "    dPdN = torch.ones_like(forward_info['N'])\n",
    "\n",
    "    dPdB = torch.ones_like(weights['B'])\n",
    "    \n",
    "    dLdN = dLdP * dPdN\n",
    "    \n",
    "    dNdW = forward_info['X'].transpose(0, 1)\n",
    "\n",
    "    dLdW = torch.mm(dNdW, dLdN)\n",
    "    \n",
    "    dLdB = dLdP * dPdB\n",
    "    \n",
    "    loss_gradients: Dict[str, Tensor] = {}\n",
    "    loss_gradients['W'] = dLdW\n",
    "    loss_gradients['B'] = torch.sum(dLdB)\n",
    "    \n",
    "    return loss_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "Batch = Tuple[Tensor, Tensor]\n",
    "\n",
    "def generate_batch(X: Tensor, \n",
    "                   y: Tensor,\n",
    "                   start: int = 0,\n",
    "                   batch_size: int = 10) -> Batch:\n",
    "    \n",
    "    assert (X.dim() == 2) and (y.dim() == 2), \\\n",
    "    \"X and Y must be 2 dimensional\"\n",
    "\n",
    "    if start+batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0] - start\n",
    "    \n",
    "    X_batch, y_batch = X[start:start+batch_size], y[start:start+batch_size]\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(X: Tensor,\n",
    "                 y: Tensor,\n",
    "                 weights: Dict[str, Tensor]) -> Tuple[Dict[str, Tensor], float]:\n",
    "\n",
    "    N = torch.mm(X, weights['W'])\n",
    "\n",
    "    P = torch.add(N, weights['B'].item())\n",
    "\n",
    "    loss = torch.sum(torch.pow(y - P, 2)).item()\n",
    "\n",
    "    forward_info: Dict[str, Tensor] = {}\n",
    "    forward_info['X'] = X\n",
    "    forward_info['N'] = N\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "\n",
    "    return forward_info, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_lr(n_in) -> Dict[str, Tensor]:\n",
    "    \n",
    "    weights: Dict[str, Tensor] = {}\n",
    "    W = torch.empty(n_in, 1).uniform_(-1, 1)\n",
    "    B = torch.empty(1, 1).uniform_(-1, 1)\n",
    "    \n",
    "    weights['W'] = W\n",
    "    weights['B'] = B\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X: Tensor, \n",
    "          y: Tensor, \n",
    "          n_iter: int = 1000,\n",
    "          learning_rate: float = 0.01,\n",
    "          batch_size: int = 100,\n",
    "          return_losses: bool = False, \n",
    "          return_weights: bool = False, \n",
    "          seed: int = 1) -> None:\n",
    "\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "    y = to_2d(y, \"col\")\n",
    "    start = 0\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = init_weights_lr(X.shape[1])\n",
    "\n",
    "    # Permute data\n",
    "    X, y = permute_data(X, y)\n",
    "    \n",
    "    if return_losses:\n",
    "        losses = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Generate batch\n",
    "        if start >= X.shape[0]:\n",
    "            X, y = permute_data(X, y)\n",
    "            start = 0\n",
    "        \n",
    "        X_batch, y_batch = generate_batch(X, y, start, batch_size)\n",
    "        start += batch_size\n",
    "    \n",
    "        # Train net using generated batch\n",
    "        forward_info, loss = forward_loss(X_batch, y_batch, weights)\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_grads = loss_gradients(forward_info, weights)\n",
    "        for key in weights.keys():\n",
    "            weights[key] -= learning_rate * loss_grads[key]\n",
    "\n",
    "    if return_weights:\n",
    "        return losses, weights\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train(X_train, y_train, \n",
    "                   learning_rate = 0.001,\n",
    "                   batch_size=23, \n",
    "                   return_losses=True, \n",
    "                   return_weights=True, \n",
    "                   seed=80718)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: Tensor, \n",
    "            y: Tensor, \n",
    "            weights: Dict[str, Tensor]):\n",
    "    \n",
    "    N = torch.mm(X, weights['W'])\n",
    "\n",
    "    return torch.add(N, weights['B'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5933895451696856"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_2(learning_rate: float = 0.01, \n",
    "             n_iter: int = 1000) -> float:\n",
    "    train_info = train(X_train, y_train, \n",
    "                       learning_rate=learning_rate,\n",
    "                       batch_size=23, \n",
    "                       n_iter=n_iter,\n",
    "                       return_losses=True, \n",
    "                       return_weights=True, \n",
    "                       seed=80718)\n",
    "\n",
    "    losses = train_info[0]\n",
    "    weights = train_info[1]\n",
    "\n",
    "    preds = predict(X_test, y_test, weights)\n",
    "    \n",
    "    return r2_score(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.geomspace(0.01, 0.00001, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_ALL:\n",
    "    r2s = [r2_score_2(float(lr), 10000) for lr in lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TEST_ALL:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.xlim(lrs[-1], lrs[0])\n",
    "    plt.semilogx(lrs, r2s);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train(X_train, y_train, \n",
    "                   learning_rate=0.0002,\n",
    "                   batch_size=23, \n",
    "                   n_iter=10000,\n",
    "                   return_losses=True, \n",
    "                   return_weights=True, \n",
    "                   seed=80718)\n",
    "\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]\n",
    "\n",
    "preds = predict(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0014],\n",
       "        [ 0.6908],\n",
       "        [ 0.2472],\n",
       "        [ 0.7149],\n",
       "        [-2.2061],\n",
       "        [ 2.3760],\n",
       "        [ 0.7110],\n",
       "        [-2.6672],\n",
       "        [ 2.5395],\n",
       "        [-1.7179],\n",
       "        [-2.3258],\n",
       "        [ 0.8528],\n",
       "        [-4.1907]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00677299,  0.70553407,  0.27897821,  0.70964421, -2.21195566,\n",
       "        2.37317966,  0.7164624 , -2.66326995,  2.62957833, -1.81593133,\n",
       "       -2.32931902,  0.85741777, -4.19602686])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.6193]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.61845081746217"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TEST_ALL:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.semilogy(list(range(10000)), losses); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: Tensor, \n",
    "            deriv: bool=False) -> Tensor:\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + torch.exp(-1.0 * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights_lr(n_in) -> Dict[str, Tensor]:\n",
    "    \n",
    "#     weights: Dict[str, Tensor] = {}\n",
    "#     W = torch.empty(n_in, 1).uniform_(-1, 1)\n",
    "#     B = torch.empty(1, 1).uniform_(-1, 1)\n",
    "    \n",
    "#     weights['W'] = W\n",
    "#     weights['B'] = B\n",
    "\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights_lrnn(input_size: int, \n",
    "#                       hidden_size: int) -> Dict[str, Tensor]:\n",
    "    \n",
    "#     weights: Dict[str, Tensor] = {}\n",
    "#     weights['W1'] = torch.randn(input_size, hidden_size)\n",
    "#     weights['B1'] = torch.randn(1, hidden_size)\n",
    "#     weights['W2'] = torch.randn(hidden_size, 1)\n",
    "#     weights['B2'] = torch.randn(1, 1)\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_lrnn(input_size: int, \n",
    "                      hidden_size: int) -> Dict[str, Tensor]:\n",
    "    \n",
    "    weights: Dict[str, Tensor] = {}\n",
    "    weights['W1'] = torch.empty(input_size, hidden_size).uniform_(-1, 1)\n",
    "    weights['B1'] = torch.empty(1, hidden_size).uniform_(-1, 1)\n",
    "    weights['W2'] = torch.empty(hidden_size, 1).uniform_(-1, 1)\n",
    "    weights['B2'] = torch.empty(1, 1).uniform_(-1, 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss_lrnn(X: Tensor,\n",
    "                      y: Tensor,\n",
    "                      weights: Dict[str, Tensor]) -> Tuple[Dict[str, Tensor], float]:\n",
    "\n",
    "    M1 = torch.mm(X, weights['W1'])\n",
    "\n",
    "    N1 = torch.add(M1, weights['B1'])\n",
    "\n",
    "    O1 = sigmoid(N1)\n",
    "    \n",
    "    M2 = torch.mm(O1, weights['W2'])\n",
    "\n",
    "    P = torch.add(M2, weights['B2'].item())    \n",
    "\n",
    "    loss = torch.sum(torch.pow(y - P, 2)).item()\n",
    "\n",
    "    forward_info: Dict[str, Tensor] = {}\n",
    "    forward_info['X'] = X\n",
    "    forward_info['M1'] = M1\n",
    "    forward_info['N1'] = N1\n",
    "    forward_info['O1'] = O1\n",
    "    forward_info['M2'] = M2\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y\n",
    "\n",
    "    return forward_info, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradients_lrnn(forward_info: Dict[str, Tensor], \n",
    "                        weights: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
    "    \n",
    "    dLdP = -(forward_info['y'] - forward_info['P'])\n",
    "    \n",
    "    dPdM2 = torch.ones_like(forward_info['M2'])\n",
    "\n",
    "    dLdM2 = dLdP * dPdM2\n",
    "  \n",
    "    dPdB2 = torch.ones_like(weights['B2'])\n",
    "\n",
    "    dLdB2 = dLdP * dPdB2\n",
    "    \n",
    "    dM2dW2 = forward_info['O1'].transpose(0, 1)\n",
    "    \n",
    "    dLdW2 = torch.mm(dM2dW2, dLdP)\n",
    "\n",
    "    dM2dO1 = weights['W2'].transpose(0, 1) \n",
    "\n",
    "    dLdO1 = torch.mm(dLdM2, dM2dO1)\n",
    "    \n",
    "    dO1dN1 = sigmoid(forward_info['N1'], deriv=True)\n",
    "    \n",
    "    dLdN1 = dLdO1 * dO1dN1\n",
    "    \n",
    "    dN1dB1 = torch.ones_like(weights['B1'])\n",
    "    \n",
    "    dN1dM1 = torch.ones_like(forward_info['M1'])\n",
    "    \n",
    "    dLdB1 = dLdN1 * dN1dB1\n",
    "    \n",
    "    dLdM1 = dLdN1 * dN1dM1\n",
    "    \n",
    "    dM1dW1 = forward_info['X'].transpose(0, 1) \n",
    "\n",
    "    dLdW1 = torch.mm(dM1dW1, dLdM1)\n",
    "    \n",
    "    def _bias_gradient(dLdB):\n",
    "        batch_sum = torch.sum(dLdB, dim=0)\n",
    "        size = batch_sum.shape[0]\n",
    "        return batch_sum.reshape(1, size)\n",
    "    \n",
    "    loss_gradients: Dict[str, Tensor] = {}\n",
    "    loss_gradients['W2'] = dLdW2\n",
    "    loss_gradients['B2'] = torch.sum(dLdB2)\n",
    "    loss_gradients['W1'] = dLdW1\n",
    "    loss_gradients['B1'] = _bias_gradient(dLdB1)\n",
    "    \n",
    "    return loss_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: Tensor, \n",
    "            y: Tensor, \n",
    "            weights: Dict[str, Tensor]):\n",
    "    \n",
    "    M1 = torch.mm(X, weights['W1'])\n",
    "\n",
    "    N1 = torch.add(M1, weights['B1'])\n",
    "\n",
    "    O1 = sigmoid(N1)\n",
    "    \n",
    "    M2 = torch.mm(O1, weights['W2'])\n",
    "\n",
    "    P = torch.add(M2, weights['B2'].item())    \n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train: Tensor, y_train: Tensor,\n",
    "          X_test: Tensor, y_test: Tensor,\n",
    "          n_iter: int = 1000,\n",
    "          test_every: int = 1000,\n",
    "          learning_rate: float = 0.01,\n",
    "          hidden_size= 13,\n",
    "          batch_size: int = 100,\n",
    "          return_losses: bool = False, \n",
    "          return_weights: bool = False, \n",
    "          return_scores: bool = False,\n",
    "          seed: int = 1) -> None:\n",
    "\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "    y_train = to_2d(y_train, \"col\")\n",
    "    y_test = to_2d(y_test, \"col\")\n",
    "    start = 0\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = init_weights_lrnn(X_train.shape[1], \n",
    "                                hidden_size=hidden_size)\n",
    "\n",
    "    # Permute data\n",
    "    X_train, y_train = permute_data(X_train, y_train)\n",
    "    \n",
    "    if return_losses:\n",
    "        losses = []\n",
    "        \n",
    "    val_scores = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Generate batch\n",
    "        if start >= X_train.shape[0]:\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            start = 0\n",
    "        \n",
    "        X_batch, y_batch = generate_batch(X_train, y_train, start, batch_size)\n",
    "        start += batch_size\n",
    "    \n",
    "        # Train net using generated batch\n",
    "        forward_info, loss = forward_loss_lrnn(X_batch, y_batch, weights)\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_grads = loss_gradients_lrnn(forward_info, weights)\n",
    "        for key in weights.keys():\n",
    "            weights[key] -= learning_rate * loss_grads[key]\n",
    "        \n",
    "        if return_scores:\n",
    "            if i % test_every == 0 and i != 0:\n",
    "                preds = predict(X_test, y_test, weights)\n",
    "                val_scores.append(r2_score(preds, y_test))\n",
    "\n",
    "    if return_weights:\n",
    "        return losses, weights, val_scores\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 200000\n",
    "test_every = 1000\n",
    "train_info = train(X_train, y_train, X_test, y_test,\n",
    "                   n_iter=num_iter,\n",
    "                   test_every = test_every,\n",
    "                   learning_rate = 0.00001,\n",
    "                   batch_size=23, \n",
    "                   return_losses=True, \n",
    "                   return_weights=True, \n",
    "                   return_scores=True,\n",
    "                   seed=80718)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]\n",
    "val_scores = train_info[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ylim([-1,1])\n",
    "plt.plot(list(range(int(num_iter / test_every - 1))), val_scores); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8372842686520985"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: Tensor, \n",
    "            deriv: bool=False) -> Tensor:\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + torch.exp(-1.0 * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: Tensor) -> Tensor:\n",
    "\n",
    "    assert x.dim() == 2, \\\n",
    "    \"Expect Tensor with shape (batch_size, num_classes), instead \" + \\\n",
    "    \"x has shape {0}\".format(x.shape)\n",
    "    \n",
    "    def _softmax_row(row: Tensor) -> Tensor:\n",
    "        \n",
    "        assert row.dim() == 1, \\\n",
    "        \"'row' should indeed be a row, instead it has shape\" \\\n",
    "        .format(row.shape)\n",
    "        \n",
    "        exp_obs = torch.exp(row)\n",
    "        sum_exp_obs = exp_obs.sum().item()\n",
    "        softmax_obs = exp_obs / sum_exp_obs\n",
    "        \n",
    "        return softmax_obs\n",
    "\n",
    "    output_rows = []\n",
    "    for obs in range(x.shape[0]):\n",
    "        output_row = to_2d(_softmax_row(x[obs]), \"row\")\n",
    "        output_rows.append(output_row)\n",
    "        \n",
    "    return torch.cat(output_rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(Tensor([[10, 8, 6, 4, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "data = breast_cancer.data\n",
    "target = breast_cancer.target\n",
    "features = breast_cancer.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=80718,\n",
    "                                                    stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression(C=10e9)\n",
    "logr.fit(X_train, y_train)\n",
    "logr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand rolled logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_2d(predictions: Tensor) -> Tensor:\n",
    "    \n",
    "    assert predictions.shape[1] == 1, \\\n",
    "    \"Expected a column for predictions, got shape: {}\".format(predictions.shape)\n",
    "    \n",
    "    inverse_predictions = 1 - predictions\n",
    "    \n",
    "    return torch.cat([predictions, inverse_predictions], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = predictions_to_2d(to_2d(y_train, \"col\")), predictions_to_2d(to_2d(y_test, \"col\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_logistic(X: Tensor,\n",
    "                     y: Tensor,\n",
    "                     W: Tensor, \n",
    "                     B: Tensor) -> Tensor:\n",
    "    \n",
    "    # For the matrix multiplication to work, \n",
    "#     assert observations.shape[1] == betas.shape[0], \\\n",
    "#     \"Dimensions of betas and feature size do not match\"\n",
    "    \n",
    "    N = torch.mm(X, W)\n",
    "    \n",
    "    O = torch.add(N, B.item())\n",
    "    \n",
    "    P = sigmoid(O)\n",
    "\n",
    "    P = predictions_to_2d(P)\n",
    "    \n",
    "    L = cross_entropy(softmax(P), y)\n",
    "    \n",
    "    forward_info: Dict[str, Tensor] = {}\n",
    "    forward_info['X'] = X\n",
    "    forward_info['W'] = W\n",
    "    forward_info['B'] = B\n",
    "    forward_info['N'] = N\n",
    "    forward_info['O'] = O\n",
    "    forward_info['P'] = P  \n",
    "    forward_info['y'] = y \n",
    "    \n",
    "    return forward_info, L.sum().item() / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions: Tensor, \n",
    "                  actual: Tensor) -> Tensor:\n",
    "    \n",
    "    assert predictions.shape == actual.shape, \\\n",
    "    \"Prediction and actual must have same shape\"\n",
    "    \n",
    "    return -1.0 * actual * torch.log(predictions) - (1.0 - actual) * torch.log(1 - predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bce_softmax_deriv(predictions: Tensor, \n",
    "                           actual: Tensor) -> Tensor:\n",
    "\n",
    "    assert predictions.shape == actual.shape, \\\n",
    "    \"Prediction and actual must have same shape\"\n",
    "\n",
    "    return to_2d((predictions - actual)[:, 0], \"col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradients_logistic(forward_info: Dict[str, Tensor]) -> Tensor:\n",
    "\n",
    "#     import pdb; pdb.set_trace()\n",
    "    dLdP = loss_bce_softmax_deriv(forward_info['P'], \n",
    "                                  forward_info['y'])\n",
    "    \n",
    "    dPdO = sigmoid(forward_info['O'], deriv=True)\n",
    "    \n",
    "    dLdO = dLdP * dPdO\n",
    "    \n",
    "    dOdB = torch.ones_like(forward_info['B'])\n",
    "    \n",
    "    dLdB = dLdO * dOdB\n",
    "    \n",
    "    dOdN = torch.ones_like(forward_info['N'])\n",
    "    \n",
    "    dLdN = dLdO * dOdN\n",
    "    \n",
    "    dNdW = forward_info['X'].transpose(0, 1)\n",
    "\n",
    "    dLdW = dNdW.mm(dLdN)\n",
    "    \n",
    "    return dLdW, dLdB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X: Tensor, \n",
    "          y: Tensor, \n",
    "          n_iter: int = 1000,\n",
    "          learning_rate: float = 0.01,\n",
    "          batch_size: int = 100,\n",
    "          return_losses: bool = False, \n",
    "          return_weights: bool = False, \n",
    "          seed: int = 1) -> None:\n",
    "\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "    start = 0\n",
    "\n",
    "    # Initialize weights\n",
    "    W = torch.empty(X.shape[1], 1).uniform_(-1, 1)\n",
    "    B = torch.empty(1, 1).uniform_(-1, 1)\n",
    "\n",
    "    # Permute data\n",
    "    X, y = permute_data(X, y)\n",
    "    \n",
    "    if return_losses:\n",
    "        losses = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        if start >= X.shape[0]:\n",
    "            X, y = permute_data(X, y)\n",
    "            start = 0\n",
    "        \n",
    "        X_batch, y_batch = generate_batch(X, y, start, batch_size)\n",
    "\n",
    "        start += batch_size\n",
    "    \n",
    "        forward_info, loss = forward_logistic(X_batch, y_batch, W, B)\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "\n",
    "        dLdW, dLdB = loss_gradients_logistic(forward_info)\n",
    "        W -= learning_rate * dLdW\n",
    "        B -= learning_rate * torch.sum(dLdB)\n",
    "    \n",
    "    if return_weights:\n",
    "        weights: Dict[str, Tensor] = {}\n",
    "        weights['W'] = W\n",
    "        weights['B'] = B\n",
    "        return losses, weights\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train(X_train, y_train, \n",
    "                   learning_rate = 0.001,\n",
    "                   batch_size=20, \n",
    "                   return_losses=True, \n",
    "                   return_weights=True, \n",
    "                   seed=80718)\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predict(X: Tensor, \n",
    "                     y: Tensor, \n",
    "                     weights: Dict[str, Tensor]):\n",
    "    \n",
    "    N = torch.mm(X, weights['W'])\n",
    "    \n",
    "    O = torch.add(N, weights['B'].item())\n",
    "    \n",
    "    P = sigmoid(O)\n",
    "\n",
    "    return P >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logistic_predict(X_test, y_test, weights)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(preds, y_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_lr(learning_rate: float = 0.01, \n",
    "                      n_iter: int = 1000) -> float:\n",
    "    train_info = train(X_train, y_train, \n",
    "                       learning_rate=learning_rate,\n",
    "                       batch_size=20, \n",
    "                       n_iter=n_iter,\n",
    "                       return_losses=True, \n",
    "                       return_weights=True, \n",
    "                       seed=80718)\n",
    "\n",
    "    losses = train_info[0]\n",
    "    weights = train_info[1]\n",
    "\n",
    "    preds = logistic_predict(X_test, y_test, weights)[:, 0]\n",
    "    \n",
    "    return accuracy_score(preds, y_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = np.geomspace(10, 10000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [accuracy_score_lr(0.01, int(it)) for it in iters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.xlim(iters[-1], lrs[0])\n",
    "plt.semilogx(iters, accs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.geomspace(0.1, 0.00001, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_lr = [accuracy_score_lr(float(lr), 1000) for lr in lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.xlim(iters[-1], lrs[0])\n",
    "plt.semilogx(lrs, accs_lr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = train(X_train, y_train, \n",
    "                   learning_rate=0.01,\n",
    "                   batch_size=20, \n",
    "                   n_iter=1000,\n",
    "                   return_losses=True, \n",
    "                   return_weights=True, \n",
    "                   seed=80718)\n",
    "\n",
    "losses = train_info[0]\n",
    "weights = train_info[1]\n",
    "\n",
    "preds = logistic_predict(X_test, y_test, weights)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_baseline_random(X: Tensor, \n",
    "                             y: Tensor,\n",
    "                             seed: int = 1):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    # Initialize weights\n",
    "    W = torch.empty(X.shape[1], 1).uniform_(-1, 1)\n",
    "    B = torch.empty(1, 1).uniform_(-1, 1)\n",
    "    \n",
    "    weights: Dict[str, Tensor] = {}\n",
    "    weights['W'] = W\n",
    "    weights['B'] = B\n",
    "\n",
    "    preds = logistic_predict(X, y, weights)[:, 0]\n",
    "    \n",
    "    return accuracy_score(preds, y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_2d = predictions_to_2d(to_2d(Tensor(target), \"col\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_baseline_random(Tensor(data), target_2d, \n",
    "                         seed=80718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum(target == 1) / len(target)\n",
    "a ** 2 + (1 - a) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
